# pyPaperFlow â€”â€”â€”â€” Automatic Paper Reading Platform

[English Version](README.md) | [Chinese Version ä¸­æ–‡ç‰ˆæœ¬](README_zh.md)

An automated platform designed to streamline the process of scientific literature reading. From retrieval and collection to structured extraction and intelligent analysis, this tool aims to assist researchers in managing and digesting large volumes of papers efficiently.

## ğŸš€ Features

- **Automated Retrieval**: Search and fetch paper metadata from PubMed/Medline.
- **Full-Text Access**: Automatically download open-access full text (XML/Text) from PMC.
- **Structured Storage**:
  - **Metadata**: Stored as detailed JSON files.
  - **Lookup Table**: A CSV-based hash table for fast indexing and management.
- **Tagging System**: Manually or programmatically tag papers to create feature vectors (e.g., `relevant=1`, `reviewed=0`).
- **CLI Tool**: A user-friendly command-line interface (`pyPaperFlow`) for all operations.

## ğŸ—ï¸ Architecture Vision

The project is designed around a 7-stage workflow:

```mermaid
flowchart TD
    A[Retrieval &<br>Collection] --> B[Processing &<br>Parsing]
    B --> C[Structured<br>Extraction]
    C --> D[Deep Encoding &<br>Vectorization]
    D --> E[Dynamic Knowledge<br>Base Storage]
    E --> F[Intelligent Interaction &<br>Discovery]
    F --> G[Final Output &<br>Internalization]

    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#ffebee
    style F fill:#f1f8e9
    
    subgraph A [Stage 1: Highly Automatable]
        direction LR
        A1[Requirement Analysis] --> A2[Platform Search]
        A2 --> A3[Initial Screening]
    end

    subgraph B [Stage 2: Highly Automatable]
        direction LR
        B1[Batch Download] --> B2[Format Parsing<br>PDF/HTML/XML]
        B2 --> B3[Text Preprocessing]
    end

    subgraph C [Stage 3: Human-AI Collaboration Core]
        direction LR
        C1[Metadata Extraction] --> C2[Core Content Extraction<br>Abstract/Methods/Conclusion]
        C2 --> C3[Relation & Viewpoint Extraction]
    end

    subgraph D [Stage 4: Fully Automatable]
        direction LR
        D1[Text Slicing] --> D2[Vector Embedding]
    end

    subgraph E [Stage 5: Fully Automatable]
        direction LR
        E1[Database Storage] --> E2[Vector Indexing]
    end

    subgraph F [Stage 6: Human-AI Collaboration Core]
        direction LR
        F1[Semantic Search] --> F2[Association Rec.] --> F3[Knowledge Graph Analysis] --> F4[Review & QA]
    end

    subgraph G [Stage 7: Human-Led]
        direction LR
        G1[Critical Reading] --> G2[Inspiration Generation] --> G3[Exp. Design &<br>Paper Writing]
    end
```

### Stage Analysis & Design Philosophy

#### Stage 1: Retrieval & Collection
The starting point of the entire workflow.
- **Manual Process**: Manually entering keywords on platforms like PubMed or Google Scholar, browsing results, and saving them.
- **Automation Entry Points**:
    - **Intelligent Retrieval Agent**: Scripts using APIs or crawlers to perform periodic automated searches based on preset keywords, journal lists, or scholar tracking.
    - **Initial Screening Algorithms**: Rule-based filtering (e.g., title terms, impact factor, date range) to sort and filter results.

#### Stage 2: Processing & Parsing
Converting raw files into computer-processable plain text and metadata.
- **Automation Entry Points**:
    - **Unified Parser**: Using tools (e.g., pdfplumber, GROBID) to extract text and charts from PDFs with high precision.
    - **Metadata Enhancement**: Automatically completing full bibliographic metadata (Title, Author, DOI, etc.) and ensuring format uniformity.

#### Stage 3: Core Information Structured Extraction
The critical leap from "Text" to "Information".
- **Automation Entry Points** (Human-AI Collaboration Core):
    - **Structured Information Extraction**: Using LLMs to act as domain experts, extracting information into fixed schemas (e.g., Problem Statement, Core Methods, Key Data, Conclusions).
    - **Relation & Viewpoint Extraction**: Identifying citation intent (support/refute) and distilling core arguments.

#### Stage 4: Deep Encoding & Vectorization
Establishing mathematical representations for information.
- **Automation Entry Points**:
    - **Text Embedding**: Using Transformer models to generate high-dimensional vectors (Embeddings) for literature.
    - **Vector Storage**: Storing vectors in specialized databases (e.g., ChromaDB, Pinecone) to enable semantic retrieval.

#### Stage 5: Dynamic Knowledge Base Storage & Indexing
The "Memory" of the system.
- **Automation Entry Points**:
    - **Multi-modal Database**: A dual-storage system combining relational databases (for structured info) and vector databases (for embeddings).
    - **Automated Indexing & Association**: Automatically establishing potential links between papers (co-citation analysis, method similarity) to build the initial edges of a knowledge graph.

#### Stage 6: Intelligent Interaction & Knowledge Discovery
Active exploration using the built knowledge base.
- **Automation Entry Points** (Human-AI Collaboration Core):
    - **Semantic Search Engine**: "Ask instead of Search" - understanding query semantics to return relevant passages.
    - **Association Recommendation & Visualization**: Recommending papers based on content similarity and visualizing the academic landscape.
    - **Intelligent QA & Review Generation**: Generating structured mini-reviews based on all literature in the database.

#### Stage 7: Final Output & Internalization
Human-led, with AI as an augmentation tool.
- **Automation Entry Points**:
    - **Assisted Writing & Citation**: Real-time recommendation of relevant citations and formatting during writing.
    - **Viewpoint Collision & Inspiration**: Presenting methodological conflicts or cross-domain associations to stimulate critical thinking.

*Currently, Stages 1, 2, and parts of 4/5 (Lite version via Tagging) are implemented.*

## ğŸ“¦ Installation

Ensure you have Python 3.9+ installed.

```bash
git clone <repository-url>
cd pyPaperFlow
pip install -e .
```

## ğŸ› ï¸ Usage

The platform provides a CLI tool named `paperflow`.

### 1. Search PubMed
Search for papers and get a list of PMIDs.

```bash
paperflow search "COVID-19 vaccine" --retmax 5
```

### 2. Fetch Papers
Fetch metadata for papers and save them to your local storage.

**By Query:**
```bash
paperflow fetch --query "COVID-19 vaccine" --batch-size 10
```

**By PMID List:**
Create a file `pmids.txt` with one PMID per line, then run:
```bash
paperflow fetch --file pmids.txt
```

### 3. Download Full Text
Download PMC full text for fetched papers (if available).

```bash
paperflow download-fulltext --pmid 34320283
```

### 4. Manage Tags (Feature Vectors)
Organize your papers by assigning tags. This creates a feature vector for each paper in the lookup table.

```bash
# Mark a paper as relevant
paperflow tag 34320283 relevant 1

# Mark a paper as read
paperflow tag 34320283 read 1
```

### 5. Query & Retrieve
Find papers based on your tags or retrieve full details.

**Query by Tags:**
```bash
# Find all relevant papers
paperflow query --tag relevant=1
```

**Get Paper Details:**
```bash
paperflow get 34320283
```

## ğŸ“‚ Data Structure

The platform uses a "Lite" storage approach:

-   **`paper_data/paper_lookup.csv`**: A lookup table acting as a local database.
    -   Rows: PMIDs.
    -   Columns: `json_path`, and dynamic tags (e.g., `relevant`, `topic_A`).
-   **`paper_data/papers/{pmid}.json`**: Detailed metadata and content for each paper.

## ğŸ“ Notes on Medline Format

The fetcher parses Medline format to extract rich metadata including:
-   **PMID**: PubMed ID
-   **DP**: Date of Publication
-   **TI**: Title
-   **AB**: Abstract
-   **FAU/AU**: Authors
-   **AD**: Affiliations
-   **PT**: Publication Type (e.g., Journal Article, Review)

## ğŸ”— References & Inspiration

-   [PubMed Research Extractor](https://github.com/Proveer/pubmed-research-extractor)
-   [BioLitMiner](https://github.com/akshayoo/BioLitMiner)


## Test Cases

### ğŸ§¬ Case 1: Get PMIDs from Query

run the command:
```bash
paperflow search "alphafold3 AND conformation AND ensemble" --email YOUR_EMAIL --api-key YOUR_NCBI_API_KEY -o ./test
```
the log shows:
```bash
âœ… NCBI API Key set successfully. Rate limit increased to 10 req/s.
Now searching PubMed with query [alphafold3 AND conformation AND ensemble] at [2025-12-19 14:41:14] ...
found 12 related articles about [alphafold3 AND conformation AND ensemble] at [2025-12-19 14:41:15] ...
Retrieving 12 PMIDs from history server at [2025-12-19 14:41:15] ...
Fetching PMIDs 1 to 12 at [2025-12-19 14:41:15] ...
  -> Retrieved 12 PMIDs in this batch.
Total PMIDs retrieved: 12 out of 12 at [2025-12-19 14:41:17] ...
Found 12 PMIDs.
['41249430', '41147497', '41014267', '40950168', '40938899', '40714407', '40549150', '40490178', '39574676', '39186607', '38996889', '38995731']
PMIDs saved to ./test/searched_pmids.txt.
```
As you can see, we will print the PMIDs list for you and save it in a text file which can be used further.


### ğŸ§¬ Case 2: Fetch Metadata for pubmed papers from query or PMIDs list

If you do not have detailed PMID list and want to fetch meta information from query, run the command:
```bash
paperflow fetch -q "alphafold3 AND conformation AND ensemble" --email YOUR_EMAIL --api-key YOUR_NCBI_API_KEY -o ./test
```

the log shows:
```bash
âœ… NCBI API Key set successfully. Rate limit increased to 10 req/s.
Fetching papers for query: alphafold3 AND conformation AND ensemble
Now searching PubMed with query [alphafold3 AND conformation AND ensemble] at [2025-12-20 18:28:05] ...
found 12 related articles about [alphafold3 AND conformation AND ensemble] at [2025-12-20 18:28:08] ...
Fetching articles 1 to 12 at [2025-12-20 18:28:08] ...
  -> Retrieved 12 Medline records and 12 Xml articles. Please check whether they equal and the efetch number here with esearch count.
Error in batch Elink acheck for 12 PMIDs (Attempt 1/5): [IncompleteRead(167 bytes read)]. Retrying in 1s...
Error in batch Elink acheck for 12 PMIDs (Attempt 2/5): [IncompleteRead(167 bytes read)]. Retrying in 1s...
Error in batch Elink acheck for 12 PMIDs (Attempt 3/5): [IncompleteRead(167 bytes read)]. Retrying in 1s...
Error in batch Elink acheck for 12 PMIDs (Attempt 4/5): [IncompleteRead(167 bytes read)]. Retrying in 1s...
    [Warning] Batch Elink acheck failed for 12 PMIDs after 5 attempts at [2025-12-20 18:29:35] ...  Skipping discovery step (using default links). Error: IncompleteRead(167 bytes read)
  -> Deep mining 5 types of internal connections for 12 PMIDs at [2025-12-20 18:29:35] ...
     Fetching pubmed_pubmed_reviews from pubmed for 12 PMIDs at [2025-12-20 18:29:35] ...
     Fetching pubmed_pubmed_refs from pubmed for 12 PMIDs at [2025-12-20 18:29:38] ...
     Fetching pubmed_pubmed_citedin from pubmed for 12 PMIDs at [2025-12-20 18:29:41] ...
     Fetching pubmed_pubmed from pubmed for 12 PMIDs at [2025-12-20 18:29:44] ...
     Fetching pubmed_pmc from pmc for 12 PMIDs at [2025-12-20 18:29:47] ...
  -> Fetching external LinkOuts (Datasets, Full Text, etc.) for 12 PMIDs at [2025-12-20 18:29:49] ...
  -> Save paper PMID 41249430 to /data2/pyPaperFlow/test/41249430.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 41147497 to /data2/pyPaperFlow/test/41147497.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 41014267 to /data2/pyPaperFlow/test/41014267.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 40950168 to /data2/pyPaperFlow/test/40950168.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 40938899 to /data2/pyPaperFlow/test/40938899.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 40714407 to /data2/pyPaperFlow/test/40714407.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 40549150 to /data2/pyPaperFlow/test/40549150.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 40490178 to /data2/pyPaperFlow/test/40490178.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 39574676 to /data2/pyPaperFlow/test/39574676.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 39186607 to /data2/pyPaperFlow/test/39186607.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 38996889 to /data2/pyPaperFlow/test/38996889.json at [2025-12-20 18:29:50] ...
  -> Save paper PMID 38995731 to /data2/pyPaperFlow/test/38995731.json at [2025-12-20 18:29:50] ...
```


Otherwise, if you have detailed PMID list,
run the command below:
```bash
paperflow fetch -f ./test/searched_pmids.txt  --email YOUR_EMAIL --api-key YOUR_NCBI_API_KEY -o ./test/paper/pmid/
```

the log shows the same way:

```bash
âœ… NCBI API Key set successfully. Rate limit increased to 10 req/s.
Fetching 12 papers from file ./test/searched_pmids.txt.
Total PMIDs to fetch: 12 at [2025-12-21 21:33:51] ...
Fetching articles 1 to 12 (PMID: ['41249430', '41147497', '41014267', '40950168', '40938899', '40714407', '40549150', '40490178', '39574676', '39186607', '38996889', '38995731']) at [2025-12-21 21:33:51] ...
  -> Retrieved 12 Medline records and 12 Xml articles. Please check whether they equal and whether they match the number of this batch.
Error in batch Elink acheck for 12 PMIDs (Attempt 1/5): [IncompleteRead(167 bytes read)]. Retrying in 1s...
Error in batch Elink acheck for 12 PMIDs (Attempt 2/5): [IncompleteRead(167 bytes read)]. Retrying in 1s...
Error in batch Elink acheck for 12 PMIDs (Attempt 3/5): [IncompleteRead(167 bytes read)]. Retrying in 1s...
Error in batch Elink acheck for 12 PMIDs (Attempt 4/5): [IncompleteRead(167 bytes read)]. Retrying in 1s...
    [Warning] Batch Elink acheck failed for 12 PMIDs after 5 attempts at [2025-12-21 21:35:19] ...  Skipping discovery step (using default links). Error: IncompleteRead(167 bytes read)
  -> Deep mining 5 types of internal connections for 12 PMIDs at [2025-12-21 21:35:19] ...
     Fetching pubmed_pubmed_reviews from pubmed for 12 PMIDs at [2025-12-21 21:35:19] ...
     Fetching pubmed_pubmed_refs from pubmed for 12 PMIDs at [2025-12-21 21:35:21] ...
     Fetching pubmed_pubmed from pubmed for 12 PMIDs at [2025-12-21 21:35:24] ...
     Fetching pubmed_pmc from pmc for 12 PMIDs at [2025-12-21 21:35:27] ...
     Fetching pubmed_pubmed_citedin from pubmed for 12 PMIDs at [2025-12-21 21:35:30] ...
  -> Fetching external LinkOuts (Datasets, Full Text, etc.) for 12 PMIDs at [2025-12-21 21:35:34] ...
  -> Save paper PMID 41249430 to /data2/pyPaperFlow/test/paper/pmid/41249430.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 41147497 to /data2/pyPaperFlow/test/paper/pmid/41147497.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 41014267 to /data2/pyPaperFlow/test/paper/pmid/41014267.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 40950168 to /data2/pyPaperFlow/test/paper/pmid/40950168.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 40938899 to /data2/pyPaperFlow/test/paper/pmid/40938899.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 40714407 to /data2/pyPaperFlow/test/paper/pmid/40714407.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 40549150 to /data2/pyPaperFlow/test/paper/pmid/40549150.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 40490178 to /data2/pyPaperFlow/test/paper/pmid/40490178.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 39574676 to /data2/pyPaperFlow/test/paper/pmid/39574676.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 39186607 to /data2/pyPaperFlow/test/paper/pmid/39186607.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 38996889 to /data2/pyPaperFlow/test/paper/pmid/38996889.json at [2025-12-21 21:35:36] ...
  -> Save paper PMID 38995731 to /data2/pyPaperFlow/test/paper/pmid/38995731.json at [2025-12-21 21:35:36] ...
```



We store the meta data of the paper in a json file.
 
One example [PMID 41249430](./test/41249430.json) listed as below:


![alt text](./figs/41249430.png)



## ğŸ“ TODOs 

å› ä¸ºè¦åšçš„å†…å®¹æ¯”è¾ƒå¤šï¼Œè¿™é‡Œè¿˜æ˜¯æŒ‰ç…§æ€»æµç¨‹è®¾è®¡æ¥ï¼Œä¹Ÿå°±æ˜¯æŒ‰ç…§æˆ‘ä»¬åŸæœ¬çš„è®¾è®¡æ­¥éª¤æ¥ï¼Œé’ˆå¯¹æ¯ä¸€æ­¥éª¤æœ‰ä»€ä¹ˆéœ€è¦å®ç°çš„

<details>
<summary><b>Stage 1: æ£€ç´¢ä¸æ”¶é›†</b></summary>

> - [ ] ç›®å‰æ–‡çŒ®æ•°æ®åº“ä»…ä»…åªè¦†ç›–äº†pubmed, å¯¹äºå…¶ä»–é¢„å°æœ¬å¹³å°çš„æ–‡çŒ®æ•°æ®åº“å¹¶ä¸æ”¯æŒ, ä½†æ˜¯ä¸€ä¸ªäººå†™è§£æå¤ªéº»çƒ¦äº†, çœ‹åˆ°æœ‰ä¸€ä¸ªéå¸¸æ£’çš„ä»“åº“, å¯ä»¥å€ŸåŠ©å…¶å¯¹äºé™¤äº†pubmedä¹‹å¤–å…¶ä»–æ•°æ®è§£æçš„éƒ¨åˆ†ï¼Œå¯ä»¥æ•´ä¸ªåº“éƒ½importè¿›æ¥, ä½œä¸ºæ•´ä¸ªä¾èµ–çš„ä¸€éƒ¨åˆ†,å°±æ˜¯å¯ä»¥å®Œå…¨ç‹¬ç«‹, â€”â€”ã€‹å£°æ˜æ˜¯å¤–éƒ¨ä¾èµ–åº“[paperscraper](https://github.com/jannisborn/paperscraper)

</details>



## ğŸ“ Storage Design
æˆ‘ä»¬å¯ä»¥å°†æ¯ä¸€ä¸ªå‡½æ•°çš„åŠŸèƒ½ä»¥åŠæ–‡ä»¶å¤¹éƒ½è‡ªç”±è®¾è®¡ï¼Œç„¶ååœ¨è¾“å‡ºçš„æ—¶å€™å†ç»Ÿä¸€æ•´ç†


ç›®å‰æœ‰ä¸¤ç§å­˜å‚¨ç­–ç•¥ï¼Œ1ç§æ˜¯ä¸€ä¸ªpaperä½œä¸º1ä¸ªæ–‡ä»¶å¤¹ï¼Œç„¶åè¿™ä¸ªæ–‡ä»¶å¤¹é‡Œé¢å­˜æ”¾è¿™ç¯‡æ–‡çŒ®çš„metadataã€full textç­‰ï¼›
å¦å¤–ä¸€ç§æŒ‰ç…§å±‚çº§åˆ†ç±»ï¼Œå°±æ˜¯metaã€full textç­‰ä½œä¸ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œç„¶åè¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹é¢æ¯ä¸€ä¸ªæ–‡çŒ®pmidæ”¾ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæˆ–è€…æ˜¯ç›´æ¥æ”¾æ–‡ä»¶




âš ï¸ æš‚æ—¶é‡‡ç”¨ç¬¬ä¸€ç§

éšç€æ•°æ®é‡çš„å¢é•¿å’Œæ•°æ®ç±»å‹çš„ä¸°å¯Œï¼ˆå…ƒæ•°æ®ã€å…¨æ–‡ã€å‘é‡ã€å›¾è°±å…³ç³»ç­‰ï¼‰ï¼Œä¸€ä¸ªæ‰å¹³çš„æ–‡ä»¶å¤¹ç»“æ„ï¼ˆæ‰€æœ‰ JSON å †åœ¨ä¸€èµ·ï¼‰å¾ˆå¿«å°±ä¼šå˜å¾—ä¸å¯ç»´æŠ¤ã€‚

æ¨èçš„æ•°æ®ä»“åº“å±‚çº§è®¾è®¡
æˆ‘å»ºè®®é‡‡ç”¨ "åˆ†å±‚ + åˆ†æ¡¶ (Sharding)" çš„æ··åˆç»“æ„ã€‚

1. é¡¶å±‚è®¾è®¡ï¼šdata_repository/
è¿™æ˜¯ä½ çš„æ•°æ®ä»“åº“æ ¹ç›®å½•ï¼Œå»ºè®®ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ Git ä»“åº“æˆ–å•ç‹¬æŒ‚è½½çš„å­˜å‚¨å·ã€‚

```bash
data_repository/
â”œâ”€â”€ metadata/               # æ ¸å¿ƒå…ƒæ•°æ® (JSON)
â”‚   â”œâ”€â”€ 00/                 # åˆ†æ¡¶ç›®å½• (åŸºäº PMID åä¸¤ä½)
â”‚   â”‚   â”œâ”€â”€ 34320200.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ fulltext/               # å…¨æ–‡æ•°æ®
â”‚   â”œâ”€â”€ pmc_xml/            # åŸå§‹ XML
â”‚   â””â”€â”€ parsed_text/        # è§£æåçš„çº¯æ–‡æœ¬
â”œâ”€â”€ vectors/                # å‘é‡æ•°æ® (å¦‚æœä¸ç”¨å‘é‡æ•°æ®åº“)
â”‚   â””â”€â”€ embeddings.npy
â”œâ”€â”€ indices/                # ç´¢å¼•ä¸æŸ¥æ‰¾è¡¨
â”‚   â”œâ”€â”€ paper_lookup.csv    # ä½ çš„æ ¸å¿ƒæŸ¥æ‰¾è¡¨
â”‚   â””â”€â”€ tag_index.json      # æ ‡ç­¾å€’æ’ç´¢å¼•
â””â”€â”€ logs/                   # è¿è¡Œæ—¥å¿—
```


2. å…³é”®å†³ç­–ç‚¹ï¼šä¸€ä¸ª PMID ä¸€ä¸ªæ–‡ä»¶å¤¹ vs åˆ†ç±»å­˜æ”¾ï¼Ÿ
æ–¹æ¡ˆ Aï¼šä»¥ PMID ä¸ºæ ¸å¿ƒçš„æ–‡ä»¶å¤¹ï¼ˆä½ æåˆ°çš„æƒ³æ³•ï¼‰

```bash
papers/
  â””â”€â”€ 34320283/
      â”œâ”€â”€ metadata.json
      â”œâ”€â”€ fulltext.txt
      â””â”€â”€ fulltext.xml

```
ä¼˜ç‚¹ï¼šç‰©ç†ä¸Šèšåˆï¼Œåˆ é™¤æŸç¯‡è®ºæ–‡æ—¶éå¸¸æ–¹ä¾¿ï¼ˆç›´æ¥åˆ æ–‡ä»¶å¤¹ï¼‰ã€‚
ç¼ºç‚¹ï¼šæ–‡ä»¶ç³»ç»Ÿå‹åŠ›å¤§ï¼ˆinode æ¶ˆè€—æ˜¯ 3 å€ï¼‰ï¼Œä¸”å½“ä½ åªæƒ³â€œéå†æ‰€æœ‰å…ƒæ•°æ®â€æ—¶ï¼Œéœ€è¦é€’å½’è¿›å…¥æ¯ä¸ªæ–‡ä»¶å¤¹ï¼ŒIO æ•ˆç‡æä½ã€‚



æ–¹æ¡ˆ Bï¼šæŒ‰æ•°æ®ç±»å‹åˆ†ç±»å­˜æ”¾ï¼ˆæ¨èæ–¹æ¡ˆï¼‰
```bash
metadata/
  â””â”€â”€ 34320283.json
fulltext/
  â””â”€â”€ 34320283.txt

```

ä¼˜ç‚¹ï¼š
æ‰¹é‡å¤„ç†æå¿«ï¼šå¦‚æœä½ è¦è®­ç»ƒ NLP æ¨¡å‹ï¼Œåªè¯» fulltext/ ç›®å½•å³å¯ï¼›å¦‚æœä½ è¦æ„å»ºå›¾è°±ï¼Œåªè¯» metadata/ å³å¯ã€‚
ç»“æ„æ¸…æ™°ï¼šä¸åŒç±»å‹çš„æ•°æ®ç”Ÿå‘½å‘¨æœŸä¸åŒï¼ˆå…ƒæ•°æ®å¯èƒ½å¸¸æ›´æ–°ï¼Œå…¨æ–‡å¯èƒ½ä¸‹è½½ä¸€æ¬¡å°±ä¸åŠ¨äº†ï¼‰ã€‚



3. è§£å†³â€œæ–‡ä»¶å¤ªå¤šâ€çš„é—®é¢˜ï¼šåˆ†æ¡¶ (Sharding)
å½“ä½ çš„æ–‡çŒ®æ•°é‡è¶…è¿‡ 10 ä¸‡ç¯‡æ—¶ï¼Œå•ç›®å½•ä¸‹æ–‡ä»¶è¿‡å¤šä¼šå¯¼è‡´ ls å¡æ­»ï¼Œæ–‡ä»¶ç³»ç»Ÿæ€§èƒ½ä¸‹é™ã€‚
è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ PMID çš„æœ€åä¸¤ä½ä½œä¸ºå­ç›®å½•ã€‚

PMID: 34320283 -> å­˜æ”¾åœ¨ .../83/34320283.json
è¿™æ ·ä½ æœ‰ 100 ä¸ªå­ç›®å½•ï¼ˆ00-99ï¼‰ï¼Œæ¯ä¸ªç›®å½•ä¸‹æ–‡ä»¶æ•°é‡å‡å°‘ 100 å€ï¼Œè½»æ¾æ”¯æŒåƒä¸‡çº§æ–‡çŒ®ã€‚










æˆ‘ä»¬åŸå§‹çš„åˆ†æå¦‚ä¸‹è§„åˆ’:


 All structure should be Raw Data -> Fetcher -> Paper Object -> JSON/Database -> AI Analyzer



ç°åœ¨è¦åšçš„:
1ï¼Œå…ˆæŠŠæˆ‘çš„å·¥å…·ä¸­èƒ½å¤Ÿåšçš„åšå¥½:
å°±æ˜¯ç›®å‰æˆ‘çš„å·¥å…·: stage1èƒ½å¤Ÿåšçš„æ¨¡å—åšå¥½

ï¼è¿˜å¾—å®ç°ï¼Œç»™pmidï¼Œè¿”å›å…¨æ–‡æ–‡æœ¬æ•°æ®


2, tagå°±æ˜¯å¯¹äºæ¯ä¸€ç¯‡æ–‡çŒ®çš„ä¸€ä¸ªlist
å¯¹äºæ¯ä¸€ç¯‡æ–‡çŒ®çš„tags, å¯ä»¥show/listï¼Œå¯ä»¥addï¼Œä¹Ÿå¯ä»¥remove

3, ä¸€äº›éœ€æ±‚å¯ä»¥å‚è€ƒï¼šhttps://github.com/andybrandt/mcp-simple-pubmed
ä¸€äº›è§£å†³æ–¹æ³•


4, å¦‚ä½•ä½¿ç”¨AIä»‹å…¥ï¼š
https://github.com/arokem/pubmed-gpt?tab=readme-ov-file
å¦‚ä½•åšæˆä¸€ä¸ªåˆ©ç”¨ç®€å•APIçš„å·¥å…·




5, æ•´ä½“æ–‡æ¡£æ³¨é‡Šé£æ ¼å¯ä»¥å‚è€ƒï¼š
https://github.com/iCodator/scientific_research_tool


6, æ•´ä½“å‚è€ƒå€ŸåŠ©llmå¯ä»¥åšåˆ°çš„æ•°æ®åº“å±‚é¢ï¼š
https://github.com/BridgesLab/ResearchAssistant
è‚¯å®šå¾—å€ŸåŠ©zotero



æ„å»ºè‡ªç„¶query:
https://github.com/iCodator/scientific_research_tool