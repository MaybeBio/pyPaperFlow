<article article-type="research-article" dtd-version="1.4" xml:lang="en" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id><journal-id journal-id-type="pmc-domain-id">716</journal-id><journal-id journal-id-type="pmc-domain">bioinfo</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-title-group><journal-title>Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1367-4811</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC11645436</article-id><article-id pub-id-type="pmcid-ver">PMC11645436.1</article-id><article-id pub-id-type="pmcaid">11645436</article-id><article-id pub-id-type="pmcaiid">11645436</article-id><article-id pub-id-type="pmid">39570595</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btae675</article-id><article-id pub-id-type="publisher-id">btae675</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Original Paper</subject><subj-group subj-group-type="category-toc-heading"><subject>Structural Bioinformatics</subject></subj-group></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI01060</subject></subj-group></article-categories><title-group><article-title>Improved prediction of post-translational modification crosstalk within proteins using DeepPCT</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Huang</surname><given-names initials="YX">Yu-Xiang</given-names></name><aff>
<institution>Hubei Key Laboratory of Agricultural Bioinformatics, College of Informatics, Huazhong Agricultural University</institution>, Wuhan 430070, <country country="CN">P.R. China</country></aff></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="false" contrib-id-type="orcid">https://orcid.org/0000-0001-9603-398X</contrib-id><name name-style="western"><surname>Liu</surname><given-names initials="R">Rong</given-names></name><aff>
<institution>Hubei Key Laboratory of Agricultural Bioinformatics, College of Informatics, Huazhong Agricultural University</institution>, Wuhan 430070, <country country="CN">P.R. China</country></aff><xref ref-type="corresp" rid="btae675-cor1"/></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Gao</surname><given-names initials="X">Xin</given-names></name><role>Associate Editor</role></contrib></contrib-group><author-notes><corresp id="btae675-cor1">Corresponding author. Hubei Key Laboratory of Agricultural Bioinformatics, College of Informatics, Huazhong Agricultural University, Wuhan 430070, P. R. China. E-mail: <email>liurong116@mail.hzau.edu.cn</email></corresp></author-notes><pub-date pub-type="collection"><month>12</month><year>2024</year></pub-date><pub-date iso-8601-date="2024-11-21" pub-type="epub"><day>21</day><month>11</month><year>2024</year></pub-date><volume>40</volume><issue>12</issue><issue-id pub-id-type="pmc-issue-id">475980</issue-id><elocation-id>btae675</elocation-id><history><date date-type="received"><day>15</day><month>8</month><year>2024</year></date><date date-type="rev-recd"><day>18</day><month>10</month><year>2024</year></date><date date-type="editorial-decision"><day>06</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>19</day><month>11</month><year>2024</year></date><date date-type="corrected-typeset"><day>02</day><month>12</month><year>2024</year></date></history><pub-history><event event-type="pmc-release"><date><day>21</day><month>11</month><year>2024</year></date></event><event event-type="pmc-live"><date><day>14</day><month>12</month><year>2024</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2024-12-21 16:25:15.810"><day>21</day><month>12</month><year>2024</year></date></event></pub-history><permissions><copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref content-type="ccbylicense" specific-use="textmining">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/" xmlns:xlink="http://www.w3.org/1999/xlink">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="btae675.pdf" xmlns:xlink="http://www.w3.org/1999/xlink"/><self-uri xlink:href="btae675.pdf" xmlns:xlink="http://www.w3.org/1999/xlink"/><abstract><title>Abstract</title><sec id="s1"><title>Motivation</title><p>Post-translational modification (PTM) crosstalk events play critical roles in biological processes. Several machine learning methods have been developed to identify PTM crosstalk within proteins, but the accuracy is still far from satisfactory. Recent breakthroughs in deep learning and protein structure prediction could provide a potential solution to this issue.</p></sec><sec id="s2"><title>Results</title><p>We proposed DeepPCT, a deep learning algorithm to identify PTM crosstalk using AlphaFold2-based structures. In this algorithm, one deep learning classifier was constructed for sequence-based prediction by combining the residue and residue pair embeddings with cross-attention techniques, while the other classifier was established for structure-based prediction by integrating the structural embedding and a graph neural network. Meanwhile, a machine learning classifier was developed using novel structural descriptors and a random forest model to complement the structural deep learning classifier. By integrating the three classifiers, DeepPCT outperformed existing algorithms in different evaluation scenarios and showed better generalizability on new data owing to its less distance dependency.</p></sec><sec id="s3"><title>Availability and implementation</title><p>Datasets, codes, and models of DeepPCT are freely accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/hzau-liulab/DeepPCT/" xmlns:xlink="http://www.w3.org/1999/xlink">https://github.com/hzau-liulab/DeepPCT/</ext-link>.</p></sec></abstract><funding-group><award-group award-type="grant"><funding-source><institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type="DOI">10.13039/501100001809</institution-id></institution-wrap></funding-source><award-id>32071249</award-id></award-group></funding-group><counts><page-count count="10"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-license-ref</meta-name><meta-value>CC BY</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec><title>1 Introduction</title><p>Post-translational modifications (PTMs) widely exist in eukaryotic proteins and play a critical role in many biological processes, such as signal transduction, cellular localization, and immune response (<xref ref-type="bibr" rid="btae675-B21">Liu <italic toggle="yes">et al.</italic> 2016</xref>, <xref ref-type="bibr" rid="btae675-B5">Chen <italic toggle="yes">et al.</italic> 2020</xref>). PTM-induced regulations refer to not only the effect of individual modification sites but also the combined influence of multiple modification sites. For instance, phosphorylation of Ser10 in histone H3 could impair the binding of effector proteins to methylated Lys9, thereby leading to different functional outcomes, which is known as the ‘methyl/phos switch’ model (<xref ref-type="bibr" rid="btae675-B7">Fischle <italic toggle="yes">et al.</italic> 2003</xref>). This dynamic interplay between PTMs within the same protein or across different interacting proteins is termed PTM crosstalk (<xref ref-type="bibr" rid="btae675-B29">Venne <italic toggle="yes">et al.</italic> 2014</xref>). Accurate identification of PTM crosstalk events could help to elucidate the complicated regulatory mechanism, providing a foundation for investigating protein functions. Although mass spectrometry techniques have been commonly used for the high-throughput detection of PTM sites, the large-scale experimental identification of PTM crosstalk remains a considerable challenge (<xref ref-type="bibr" rid="btae675-B32">Witze <italic toggle="yes">et al.</italic> 2007</xref>, <xref ref-type="bibr" rid="btae675-B2">Adoni <italic toggle="yes">et al.</italic> 2022</xref>). Accordingly, there is a pressing need to develop computational approaches to predict crosstalk events.</p><p>During the past two decades, a series of computational studies have been devoted to the characterization and identification of PTM crosstalk. Beltrao <italic toggle="yes">et al.</italic> discovered that the functional importance of PTMs is associated with their conservation, which could be used to identify regulatory regions within protein families (<xref ref-type="bibr" rid="btae675-B3">Beltrao <italic toggle="yes">et al.</italic> 2013</xref>). Schweiger <italic toggle="yes">et al.</italic> found that cooperative phosphorylation sites form clusters in the protein sequence, while Korkuc <italic toggle="yes">et al.</italic> showed that these sites are spatially adjacent to each other (<xref ref-type="bibr" rid="btae675-B26">Schweiger and Linial 2010</xref>, <xref ref-type="bibr" rid="btae675-B15">Korkuc and Walther 2016</xref>, <xref ref-type="bibr" rid="btae675-B16">2017</xref>). <xref ref-type="bibr" rid="btae675-B33">Woodsmith <italic toggle="yes">et al.</italic> (2013)</xref> and <xref ref-type="bibr" rid="btae675-B24">Pejaver <italic toggle="yes">et al.</italic> (2014)</xref> revealed that PTM-enriched sequence segments are highly related to intrinsically disordered regions. Based on these findings, <xref ref-type="bibr" rid="btae675-B10">Huang <italic toggle="yes">et al.</italic> (2015)</xref> developed PTM-X, which is the first machine learning algorithm to predict PTM crosstalk within proteins. They used the distance measures, co-evolutionary signals, and co-localization attributes of PTM pairs as the input of a naive Bayes classifier. PTM-X strongly depended on residue pair-based features and sequence information. Five years ago, we developed the PCTpred method to explore the utility of residue-based features and structural information (<xref ref-type="bibr" rid="btae675-B20">Liu and Liu 2020</xref>). This model included a structural classifier and a sequence classifier, both of which were implemented by combining a group of residue and residue pair-based features with random forest algorithms. Recently, <xref ref-type="bibr" rid="btae675-B6">Deng <italic toggle="yes">et al.</italic> (2023)</xref> designed a structure-based algorithm by incorporating protein dynamic features. Moreover, they explored convolutional neural networks and long-short term memory networks to predict PTM crosstalk, but the deep learning model performed worse than the random forest model.</p><p>Despite the advancements achieved by existing studies, several problems could be further explored. First, the application of deep learning techniques to predict PTM crosstalk remains challenging due to the scarcity of experimentally validated data. It is highly needed to design suitable deep leaning strategies to learn complex patterns and correlations from very limited training data. Second, incorporation of structural features could enhance the performance, but the structural information of numerous samples is unavailable. With the recent advances in protein structure prediction (e.g. AlphaFold series) (<xref ref-type="bibr" rid="btae675-B13">Jumper <italic toggle="yes">et al.</italic> 2021</xref>), the high-confidence predicted structures could be used to substitute native counterparts. Third, existing methods only adopted hand-crafted sequence and structural features in this prediction task. The embedding generated from pretrained models exhibited better performance in various functional prediction problems and could thus be extended to the identification of PTM crosstalk (<xref ref-type="bibr" rid="btae675-B12">Jiang <italic toggle="yes">et al.</italic> 2023</xref>, <xref ref-type="bibr" rid="btae675-B35">Zeng <italic toggle="yes">et al.</italic> 2023</xref>, <xref ref-type="bibr" rid="btae675-B18">Kulmanov <italic toggle="yes">et al.</italic> 2024</xref>). Fourth, all the previous studies lacked an independent testing. Especially, PCTpred and Deng <italic toggle="yes">et al.</italic>’s model were optimized on the whole data using the feature selection algorithm, which could result in overfitting issues (<xref ref-type="bibr" rid="btae675-B20">Liu and Liu 2020</xref>, <xref ref-type="bibr" rid="btae675-B6">Deng <italic toggle="yes">et al.</italic> 2023</xref>). The proposed model should be rigorously assessed using external data to ensure its generalizability.</p><p>Inspired by the above problems, we present a novel deep learning framework, named DeepPCT, to identify PTM crosstalk events within proteins. In this algorithm, one deep learning classifier was built for sequence-based prediction by combining the residue and residue pair embeddings with cross-attention techniques, while the other classifier was established for structure-based prediction by integrating the structural embedding and a graph neural network. Meanwhile, a machine learning classifier was established using a series of novel structural descriptors and a random forest model to complement the structural deep learning classifier. By integrating the three classifiers, DeepPCT not only outperformed existing methods for both sample- and protein-based evaluation but also showed better generalizability on new test data due to its less distance dependency. Especially, the inference time of DeepPCT was 60 times faster than that of our previous model.</p></sec><sec><title>2 Materials and methods</title><sec><title>2.1 Data collection</title><p>In this study, the experimentally identified PTM crosstalk pairs within proteins were collected from previous works and relevant literatures. The dataset of our earlier study, which contained 205 crosstalk pairs and 12,468 negative samples from 81 proteins, was used for model training in the current study. The 205 positive samples, including 193 pairs collected by Huang <italic toggle="yes">et al.</italic> and 12 pairs collected by our group, were reported in the literature prior to September 2017 (<xref ref-type="bibr" rid="btae675-B10">Huang <italic toggle="yes">et al.</italic> 2015</xref>, <xref ref-type="bibr" rid="btae675-B20">Liu and Liu 2020</xref>). Moreover, we used the keyword combination ‘(cross AND (talk OR regulate OR link)) OR interplay OR interaction) AND post translational modification’ to retrieve the references between September 2017 and April 2023 from the PubMed database. By reading the resultant 898 articles, we additionally obtained 20 crosstalk pairs from 9 human proteins (<xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref>). Following the previous studies, the PTM sites detected by low-throughout experiments in the PhosphoSitePlus database were used to generate negative samples (<xref ref-type="bibr" rid="btae675-B9">Hornbeck <italic toggle="yes">et al.</italic> 2015</xref>). We generated all possible PTM pairs within each protein and deleted pairs whose both sites were included in the positive set, resulting in 427 negative samples. The newly collected samples were used for independent testing. To investigate the impact of distances between PTM sites on prediction performance, we prepared another three datasets (i.e. Set-random, Seq-control, and Str-control), in which all the positive samples and an equal number of negative samples were chosen from the training set. For Set-random, the negative samples were randomly selected based on the native distance distribution. For Seq-control and Str-control, the negative samples were chosen based on the distributions of sequence and structural distances of positive samples, respectively (<xref ref-type="supplementary-material" rid="sup1">Supplementary Text S1</xref>). For the aforementioned 90 proteins, the predicted structures were used for training and testing. The structures generated by AlphaFold2 were downloaded from AlphaFold DB (<xref ref-type="bibr" rid="btae675-B27">Varadi <italic toggle="yes">et al.</italic> 2022</xref>).</p></sec><sec><title>2.2 Overview of DeepPCT</title><p>As shown in <xref ref-type="fig" rid="btae675-F1">Fig. 1</xref>, the DeepPCT algorithm is composed of a sequence-based module (DeepPCTseq), a structure-based module (DeepPCTstr), and an integration module. For a query PTM pair, the structure of the involving protein was predicted by AlphaFold2 (<xref ref-type="bibr" rid="btae675-B13">Jumper <italic toggle="yes">et al.</italic> 2021</xref>). Sequence- and structure-based predictions were then conducted using DeepPCTseq and DeepPCTstr, respectively. Regarding the sequence component, the representations of individual residues and relevant residue pairs were generated by a pretrained protein language model to capture the sequential pattern and association of two PTM sites. The residue embedding was processed by a cross-attention model and then combined with the residue pair embedding to calculate the crosstalk probability using a fully connected layer. As for the structural section, DeepPCTstr included two submodules: DeepPCTgraph and DeepPCTsite. The former generated the structural embedding of each residue through a pretrained protein structural model. A graph neural network was then applied to the graph representations of each PTM pair for evaluating the crosstalk probability. Meanwhile, the latter generated comprehensive structural descriptors to describe two PTM sites and adopted random forests to assess their crosstalk probability (<xref ref-type="bibr" rid="btae675-B4">Breiman 2001</xref>). To utilize the interplay of three classifiers, their outputs were integrated to generate the final prediction score using a weighted combination strategy.</p><fig id="btae675-F1" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>Flowchart of DeepPCT for PTM crosstalk prediction. Our algorithm includes a sequence-based module (DeepPCTseq), a structure-based module (DeepPCTstr), and an integration module. DeepPCTseq combines the residue and residue pair embeddings with cross-attention techniques to predict crosstalk events. DeepPCTstr comprises two submodules, namely DeepPCTgraph and DeepPCTsite. The former is dependent on the structural embedding and a graph neural network, and the latter is based on a group of novel structural descriptors and a random forest model. The integration module is the weighted sum of the outputs from the above three classifiers.</p></caption><graphic orientation="portrait" position="float" xlink:href="btae675f1.jpg" xmlns:xlink="http://www.w3.org/1999/xlink"/></fig></sec><sec><title>2.3 Feature extraction</title><sec><title>2.3.1 Sequence-based features</title><sec><title>2.3.1.1 Residue embedding</title><p>Trained on a large number of protein sequences through the unsupervised learning approach, protein language models (PLMs) could effectively capture the latent information within protein sequences. In this study, we adopted the ESM-2 for residue encoding by comparing different PLMs (<xref ref-type="supplementary-material" rid="sup1">Supplementary Text S2</xref> and <xref ref-type="supplementary-material" rid="sup1">Table S2</xref>). The representations from the last layer of this model were extracted as residue embedding (<xref ref-type="bibr" rid="btae675-B19">Lin <italic toggle="yes">et al.</italic> 2023</xref>).</p></sec><sec><title>2.3.1.2 Residue pair embedding</title><p>ESM-2 was also used to generate embedding features for residue pairs. This model comprises 33 transformer encoder layers, each of which contains 20 attention heads in the multi-head attention block. To generate residue pair embedding, the attention map (i.e. <inline-formula id="IE1"><mml:math display="inline" id="IM1" overflow="scroll"><mml:mtext>Softmax</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>Q</mml:mi><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfenced></mml:math></inline-formula> in the attention formula) was extracted from each attention head. For a sequence, a total of 660 (33 × 20) attention maps could be generated. The attention matrix <bold><italic toggle="yes">A</italic></bold> has a dimension of <inline-formula id="IE2"><mml:math display="inline" id="IM2" overflow="scroll"><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:math></inline-formula>, where <italic toggle="yes">L</italic> represents the sequence length. Each attention score in the matrix evaluates the dependency between two residues, which could be a potential indicator of related PTM sites. For a given sample <inline-formula id="IE3"><mml:math display="inline" id="IM3" overflow="scroll"><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, because <inline-formula id="IE4"><mml:math display="inline" id="IM4" overflow="scroll"><mml:mi>A</mml:mi></mml:math></inline-formula> is asymmetric, two attention scores, <inline-formula id="IE5"><mml:math display="inline" id="IM5" overflow="scroll"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE6"><mml:math display="inline" id="IM6" overflow="scroll"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, should be extracted. Finally, the attention scores from the 660 attention maps constituted a 1320-dimensional vector for each pair.</p></sec></sec><sec><title>2.3.2 Structure-based features</title><sec><title>2.3.2.1 Structural embedding</title><p>The structural representations of residues could also be generated by pre-trained protein models. Here, we employed the GearNet-Edge model to convert a protein structure into the graph representation of residues (nodes) (<xref ref-type="bibr" rid="btae675-B37">Zhang <italic toggle="yes">et al.</italic> 2023</xref>). The node features derived from the resulting graph were used as structural embeddings to predict PTM crosstalk.</p></sec><sec><title>2.3.2.2 Structural descriptors</title><p>A group of geometry- and graph-based descriptors were adopted to characterize the structural context of each residue that may not be fully represented by structural embeddings. Among the geometry-based descriptors, the circular variance reflects the density of neighboring atoms of a residue. The Osipov-Pickup-Dunmur (OPD) chirality index illustrates the stereochemical property of a residue’s local context and could serve as an indicator of secondary structure. Besides, three local geometric descriptors, including the accessible shell volume, minimum inaccessible radius, and pocketness, quantify the depth and exposure of residues. The graph-based descriptors were used to depict the topological properties of residues in a network. The Ollivier-Ricci curvature measures the geodesic path complexity of a residue based on Riemannian geometry, while the multifractal dimension measures the network complexity in multifractal analysis. The shortest path distance could quantify the spatial proximity of two residues. Detailed information about these features is provided in <xref ref-type="supplementary-material" rid="sup1">Supplementary Text S3</xref> and <xref ref-type="supplementary-material" rid="sup1">Table S3</xref>.</p></sec></sec></sec><sec><title>2.4 Model construction</title><sec><title>2.4.1 Sequence-based module</title><p>For a PTM pair <inline-formula id="IE7"><mml:math display="inline" id="IM7" overflow="scroll"><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, this module took two sequence windows, namely <inline-formula id="IE8"><mml:math display="inline" id="IM8" overflow="scroll"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="IE9"><mml:math display="inline" id="IM9" overflow="scroll"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>, along with a residue pair embedding <inline-formula id="IE10"><mml:math display="inline" id="IM10" overflow="scroll"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> as input. Each sequence window was a matrix composed of residue embeddings of 11 consecutive residues centered on the PTM site. Zero-padding was used to represent the missing residues if the target site appeared at both ends of a sequence. A two-layer cross-attention network was adopted to convert the two sequence windows into a merged representation (<xref ref-type="bibr" rid="btae675-B28">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>). Specifically, a residual connection followed by layer normalization was applied to each network layer as follows (<xref ref-type="bibr" rid="btae675-B8">He <italic toggle="yes">et al.</italic> 2016</xref>):
<disp-formula id="E1"><mml:math display="block" id="M1" overflow="scroll"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>LayerNorm</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></disp-formula>where <inline-formula id="IE11"><mml:math display="inline" id="IM11" overflow="scroll"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> denotes the output from the <inline-formula id="IE12"><mml:math display="inline" id="IM12" overflow="scroll"><mml:mi>l</mml:mi></mml:math></inline-formula>th layer, and <inline-formula id="IE13"><mml:math display="inline" id="IM13" overflow="scroll"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> is the initial input <inline-formula id="IE14"><mml:math display="inline" id="IM14" overflow="scroll"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>. In the layer function <inline-formula id="IE15"><mml:math display="inline" id="IM15" overflow="scroll"><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>, five parallel attention heads were used to depict interactions between sequence windows. To this end, <inline-formula id="IE16"><mml:math display="inline" id="IM16" overflow="scroll"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> was transformed into the query (<inline-formula id="IE17"><mml:math display="inline" id="IM17" overflow="scroll"><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>), while <inline-formula id="IE18"><mml:math display="inline" id="IM18" overflow="scroll"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> was transformed into the key (<inline-formula id="IE19"><mml:math display="inline" id="IM19" overflow="scroll"><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>) and value (<inline-formula id="IE20"><mml:math display="inline" id="IM20" overflow="scroll"><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>):
<disp-formula id="E2"><mml:math display="block" id="M2" overflow="scroll"><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula><disp-formula id="E3"><mml:math display="block" id="M3" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>Softmax</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:math></disp-formula>where <inline-formula id="IE21"><mml:math display="inline" id="IM21" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, <inline-formula id="IE22"><mml:math display="inline" id="IM22" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, and <inline-formula id="IE23"><mml:math display="inline" id="IM23" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> denote learnable weight matrices, and <inline-formula id="IE24"><mml:math display="inline" id="IM24" overflow="scroll"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, <inline-formula id="IE25"><mml:math display="inline" id="IM25" overflow="scroll"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, and <inline-formula id="IE26"><mml:math display="inline" id="IM26" overflow="scroll"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> are biases. The scaling factor <inline-formula id="IE27"><mml:math display="inline" id="IM27" overflow="scroll"><mml:mi>d</mml:mi></mml:math></inline-formula> was set to 256. <inline-formula id="IE28"><mml:math display="inline" id="IM28" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> represents the output of the <inline-formula id="IE29"><mml:math display="inline" id="IM29" overflow="scroll"><mml:mi>m</mml:mi></mml:math></inline-formula>th attention head in the <inline-formula id="IE30"><mml:math display="inline" id="IM30" overflow="scroll"><mml:mi>l</mml:mi></mml:math></inline-formula>th layer. The outputs of these attention heads were merged as follows:
<disp-formula id="E4"><mml:math display="block" id="M4" overflow="scroll"><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>≔</mml:mo><mml:mtext>concatenate</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula>where <inline-formula id="IE31"><mml:math display="inline" id="IM31" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a projection matrix, and <inline-formula id="IE32"><mml:math display="inline" id="IM32" overflow="scroll"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a learnable bias. The final layer’s output <inline-formula id="IE33"><mml:math display="inline" id="IM33" overflow="scroll"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> contained 11 vectors and was the fused representation of two sequence windows. The central vector <inline-formula id="IE34"><mml:math display="inline" id="IM34" overflow="scroll"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> was combined with the pair embedding <inline-formula id="IE35"><mml:math display="inline" id="IM35" overflow="scroll"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> to form a concatenated vector <inline-formula id="IE36"><mml:math display="inline" id="IM36" overflow="scroll"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, which was then fed into a three-layer fully connected network to yield the output probability as follows:
<disp-formula id="E5"><mml:math display="block" id="M5" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>Softmax</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mtext>LeakyReLU</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mtext>BatchNorm</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula>where <inline-formula id="IE37"><mml:math display="inline" id="IM37" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula id="IE38"><mml:math display="inline" id="IM38" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula id="IE39"><mml:math display="inline" id="IM39" overflow="scroll"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula id="IE40"><mml:math display="inline" id="IM40" overflow="scroll"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are learnable parameters. We implemented this module using PyTorch (<xref ref-type="bibr" rid="btae675-B22">Paszke <italic toggle="yes">et al.</italic> 2019</xref>). Early stopping was adopted if a notable decrease in performance was observed. The cross-entropy loss was used for loss calculation, and the parameters were updated using the Adam optimizer with a learning rate of 1e−5 (<xref ref-type="bibr" rid="btae675-B14">Kingma and Ba 2014</xref>).</p></sec><sec><title>2.4.2 Structure-based module</title><sec><title>2.4.2.1 Graph-based submodule</title><p>To develop this submodule, we extracted the spatial microenvironment <inline-formula id="IE41"><mml:math display="inline" id="IM41" overflow="scroll"><mml:mi>S</mml:mi></mml:math></inline-formula> of each PTM site, which included the target residue and its 10 nearest residues in terms of structural distances. For a PTM pair <inline-formula id="IE42"><mml:math display="inline" id="IM42" overflow="scroll"><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, as illustrated in <xref ref-type="fig" rid="btae675-F1">Fig. 1</xref>, their microenvironments were converted into a graph representation <inline-formula id="IE43"><mml:math display="inline" id="IM43" overflow="scroll"><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, where each node denotes a residue, and an edge is formed between (1) PTM sites <inline-formula id="IE44"><mml:math display="inline" id="IM44" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula id="IE45"><mml:math display="inline" id="IM45" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula>; (2) PTM site <inline-formula id="IE46"><mml:math display="inline" id="IM46" overflow="scroll"><mml:mi>i</mml:mi></mml:math></inline-formula> (or <inline-formula id="IE47"><mml:math display="inline" id="IM47" overflow="scroll"><mml:mi>j</mml:mi></mml:math></inline-formula>) and residues in its microenvironment <inline-formula id="IE48"><mml:math display="inline" id="IM48" overflow="scroll"><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> (or <inline-formula id="IE49"><mml:math display="inline" id="IM49" overflow="scroll"><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>); (3) two residues within <inline-formula id="IE50"><mml:math display="inline" id="IM50" overflow="scroll"><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> (or <inline-formula id="IE51"><mml:math display="inline" id="IM51" overflow="scroll"><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>) that are chemically interconnected. The structural embedding treated with layer normalization was adopted as the node feature. This graph was fed into a two-layer graph isomorphism network (GIN) for updating its node feature (<xref ref-type="bibr" rid="btae675-B34">Xu <italic toggle="yes">et al.</italic> 2019</xref>). Let <inline-formula id="IE52"><mml:math display="inline" id="IM52" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> be the initial feature of node <inline-formula id="IE53"><mml:math display="inline" id="IM53" overflow="scroll"><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:math></inline-formula>, and the updating process can be presented as follows:
<disp-formula id="E6"><mml:math display="block" id="M6" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="E7"><mml:math display="block" id="M7" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>≔</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></disp-formula>where <inline-formula id="IE54"><mml:math display="inline" id="IM54" overflow="scroll"><mml:mi>N</mml:mi><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is a set of nodes adjacent to <inline-formula id="IE55"><mml:math display="inline" id="IM55" overflow="scroll"><mml:mi>v</mml:mi></mml:math></inline-formula>, and <inline-formula id="IE56"><mml:math display="inline" id="IM56" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> is the apply function in the <inline-formula id="IE57"><mml:math display="inline" id="IM57" overflow="scroll"><mml:mi>l</mml:mi></mml:math></inline-formula>th layer, which contains the learnable parameters <inline-formula id="IE58"><mml:math display="inline" id="IM58" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="IE59"><mml:math display="inline" id="IM59" overflow="scroll"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula>. Subsequently, a readout function was used to obtain the graph representation <inline-formula id="IE60"><mml:math display="inline" id="IM60" overflow="scroll"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as follows:
<disp-formula id="E8"><mml:math display="block" id="M8" overflow="scroll"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>readout</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>≔</mml:mo><mml:mtext>concatenate</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></disp-formula>where <inline-formula id="IE61"><mml:math display="inline" id="IM61" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is a learnable projection matrix. Finally, <inline-formula id="IE62"><mml:math display="inline" id="IM62" overflow="scroll"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was input into a linear layer to generate a prediction probability as follows:
<disp-formula id="E9"><mml:math display="block" id="M9" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>graph</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>Softmax</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mtext>LeakyReLU</mml:mtext><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></disp-formula>where <inline-formula id="IE63"><mml:math display="inline" id="IM63" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="IE64"><mml:math display="inline" id="IM64" overflow="scroll"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> are learnable parameters. This model was implemented using the DGL and PyTorch (<xref ref-type="bibr" rid="btae675-B22">Paszke <italic toggle="yes">et al.</italic> 2019</xref>, <xref ref-type="bibr" rid="btae675-B31">Wang <italic toggle="yes">et al.</italic> 2019</xref>). Early stopping was used in the training process. The cross-entropy loss was used as the loss function. To avoid overfitting, we incorporated the flooding technique into our loss function, in which the flood level (<inline-formula id="IE65"><mml:math display="inline" id="IM65" overflow="scroll"><mml:mi>b</mml:mi></mml:math></inline-formula>) was set to 0.017 (<xref ref-type="bibr" rid="btae675-B11">Ishida <italic toggle="yes">et al.</italic> 2020</xref>). The Adam optimizer with a learning rate of 5e−6 was adopted for model optimization (<xref ref-type="bibr" rid="btae675-B14">Kingma and Ba 2014</xref>).</p></sec><sec><title>2.4.2.2 Site-based submodule</title><p>This submodule used hand-crafted structural descriptors as the input of random forest algorithms (<xref ref-type="bibr" rid="btae675-B4">Breiman 2001</xref>). Among these descriptors, the shortest path distance was a residue pair-based descriptor, and the remaining features were residue-based descriptors. The maximum, minimum and average values of both PTM sites were computed for residue-based descriptors. We adopted the scikit-learn package to implement this model with a configuration of 500 trees (<xref ref-type="bibr" rid="btae675-B23">Pedregosa <italic toggle="yes">et al.</italic> 2011</xref>).</p></sec></sec><sec><title>2.4.3 Integration module</title><p>We used a weighted combination method to integrate the results of different modules. Specifically, the output probability of the structure-based module (<inline-formula id="IE66"><mml:math display="inline" id="IM66" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>str</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>) was the weighted sum of probabilities from the graph- and site-based submodules (<inline-formula id="IE67"><mml:math display="inline" id="IM67" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>graph</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE68"><mml:math display="inline" id="IM68" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>site</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, respectively). The output probability of our final model (<inline-formula id="IE69"><mml:math display="inline" id="IM69" overflow="scroll"><mml:mi>P</mml:mi></mml:math></inline-formula>) was the weighted sum of probabilities from the sequence- and structure-based modules (<inline-formula id="IE70"><mml:math display="inline" id="IM70" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE71"><mml:math display="inline" id="IM71" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>str</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, respectively). The formulas are shown as follows:
<disp-formula id="E10"><mml:math display="block" id="M10" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>str</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>graph</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>site</mml:mtext></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="E11"><mml:math display="block" id="M11" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced close=")" open="(" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mtext>str</mml:mtext></mml:mrow></mml:msub></mml:math></disp-formula>where <inline-formula id="IE72"><mml:math display="inline" id="IM72" overflow="scroll"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula id="IE73"><mml:math display="inline" id="IM73" overflow="scroll"><mml:mi>β</mml:mi></mml:math></inline-formula> are the weight factors. Due to the imbalanced ratio between positive and negative samples, the cutoff of our method is relatively small. The optimal values for <inline-formula id="IE74"><mml:math display="inline" id="IM74" overflow="scroll"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula id="IE75"><mml:math display="inline" id="IM75" overflow="scroll"><mml:mi>β</mml:mi></mml:math></inline-formula>, and the cutoff are 0.65, 0.65, and 0.15, respectively (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S1</xref>).</p></sec></sec><sec><title>2.5 Performance evaluation</title><p>To evaluate our algorithm, we conducted 10-fold cross-validation (CV) on the training set and independent testing on the newly collected samples. When performing 10-fold CV, we adopted sample- and protein-based evaluations. The former randomly divided all samples into different subsets for training and validation, and the latter allocated proteins into the subsets to ensure that test samples were derived from the unseen proteins in the training process. The hyperparameters of our algorithm were determined by protein-based evaluations, because this strategy is more rigorous and closer to the real application. To train deep learning models, we used all the positive and negative samples in the training set. Seven widely used measures, including the area under the receiver operating characteristic (ROC) curve (AUC), area under the precision-recall curve (AUPR), Matthew’s correlation coefficient (MCC), F1-score, recall, precision, and accuracy (ACC), were used for assessing the performance of our model. We also evaluated significant differences in performance between different models. For a given dataset, we randomly chosen 70% of the crosstalk and negative pairs 10 times and calculated AUC and AUPR values. The Anderson-Darling test was then used to assess whether these values obey a normal distribution. Based on the normality assumption, the paired <italic toggle="yes">t</italic>-test or Wilcoxon rank-sum test was selected for statistical testing.</p></sec></sec><sec><title>3 Results</title><sec><title>3.1 Effectiveness of three basic classifiers</title><p>We evaluated the performance of three basic classifiers on the training set using 10-fold CV. As shown in <xref ref-type="table" rid="btae675-T1">Table 1</xref>, DeepPCTseq performed most favorably for sample-based evaluation, with an AUC and AUPR of 0.946 and 0.643, and outperformed two structure-based classifiers (i.e. DeepPCTsite and DeepPCTgraph) by approximately 0.05 and 0.04 in AUC and 0.3 and 0.2 in AUPR, respectively. Regarding the more rigorous protein-based evaluation, we observed a notable decrease in the performance of three classifiers. Even so, the sequence-based model still surpassed structure-based counterparts. DeepPCTsite and DeepPCTgraph achieved a more remarkable decrease in AUC from 0.894 and 0.911 to 0.736 and 0.739, respectively, while the measure of DeepPCTseq decreased from 0.946 to 0.827. This suggests that sequence information is more crucial and exhibits greater robustness in predicting PTM crosstalk compared to structural information.</p><table-wrap id="btae675-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Performance of different modules on training set using sample- and protein-based evaluation.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/></colgroup><thead><tr><th colspan="1" rowspan="2">Classifier</th><th align="center" colspan="4" rowspan="1">Sample-based evaluation<hr/></th><th align="center" colspan="4" rowspan="1">Protein-based evaluation<hr/></th></tr><tr><th align="center" colspan="1" rowspan="1">AUC</th><th align="center" colspan="1" rowspan="1">AUPR</th><th align="center" colspan="1" rowspan="1">MCC</th><th align="center" colspan="1" rowspan="1">F1</th><th align="center" colspan="1" rowspan="1">AUC</th><th align="center" colspan="1" rowspan="1">AUPR</th><th align="center" colspan="1" rowspan="1">MCC</th><th align="center" colspan="1" rowspan="1">F1</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">DeepPCTsite</td><td colspan="1" rowspan="1">0.894</td><td colspan="1" rowspan="1">0.302</td><td colspan="1" rowspan="1">0.338</td><td colspan="1" rowspan="1">0.280</td><td colspan="1" rowspan="1">0.736</td><td colspan="1" rowspan="1">0.149</td><td colspan="1" rowspan="1">0.138</td><td colspan="1" rowspan="1">0.115</td></tr><tr><td colspan="1" rowspan="1">DeepPCTsite<sup>AF3</sup></td><td colspan="1" rowspan="1">0.889</td><td colspan="1" rowspan="1">0.290</td><td colspan="1" rowspan="1">0.344</td><td colspan="1" rowspan="1">0.336</td><td colspan="1" rowspan="1">0.744</td><td colspan="1" rowspan="1">0.163</td><td colspan="1" rowspan="1">0.174</td><td colspan="1" rowspan="1">0.193</td></tr><tr><td colspan="1" rowspan="1">DeepPCTgraph</td><td colspan="1" rowspan="1">0.911</td><td colspan="1" rowspan="1">0.493</td><td colspan="1" rowspan="1">0.476</td><td colspan="1" rowspan="1">0.435</td><td colspan="1" rowspan="1">0.739</td><td colspan="1" rowspan="1">0.182</td><td colspan="1" rowspan="1">0.167</td><td colspan="1" rowspan="1">0.183</td></tr><tr><td colspan="1" rowspan="1">DeepPCTgraph<sup>AF3</sup></td><td colspan="1" rowspan="1">0.924</td><td colspan="1" rowspan="1">0.538</td><td colspan="1" rowspan="1">0.548</td><td colspan="1" rowspan="1">0.526</td><td colspan="1" rowspan="1">0.747</td><td colspan="1" rowspan="1">0.189</td><td colspan="1" rowspan="1">0.179</td><td colspan="1" rowspan="1">0.158</td></tr><tr><td colspan="1" rowspan="1">DeepPCTstr</td><td colspan="1" rowspan="1">0.942</td><td colspan="1" rowspan="1">0.525</td><td colspan="1" rowspan="1">0.503</td><td colspan="1" rowspan="1">0.471</td><td colspan="1" rowspan="1">0.786</td><td colspan="1" rowspan="1">0.217</td><td colspan="1" rowspan="1">0.211</td><td colspan="1" rowspan="1">0.228</td></tr><tr><td colspan="1" rowspan="1">DeepPCTstr<sup>AF3</sup></td><td colspan="1" rowspan="1">0.947</td><td colspan="1" rowspan="1">0.572</td><td colspan="1" rowspan="1">0.557</td><td colspan="1" rowspan="1">0.553</td><td colspan="1" rowspan="1">0.799</td><td colspan="1" rowspan="1">0.224</td><td colspan="1" rowspan="1">0.222</td><td colspan="1" rowspan="1">0.242</td></tr><tr><td colspan="1" rowspan="1">DeepPCTseq<sup>nrp</sup></td><td colspan="1" rowspan="1">0.925</td><td colspan="1" rowspan="1">0.586</td><td colspan="1" rowspan="1">0.569</td><td colspan="1" rowspan="1">0.556</td><td colspan="1" rowspan="1">0.775</td><td colspan="1" rowspan="1">0.269</td><td colspan="1" rowspan="1">0.222</td><td colspan="1" rowspan="1">0.215</td></tr><tr><td colspan="1" rowspan="1">DeepPCTseq</td><td colspan="1" rowspan="1">0.946</td><td colspan="1" rowspan="1">0.643</td><td colspan="1" rowspan="1">0.596</td><td colspan="1" rowspan="1">0.587</td><td colspan="1" rowspan="1">0.827</td><td colspan="1" rowspan="1">0.368</td><td colspan="1" rowspan="1">0.318</td><td colspan="1" rowspan="1">0.271</td></tr><tr><td colspan="1" rowspan="1">DeepPCT</td><td colspan="1" rowspan="1">0.957</td><td colspan="1" rowspan="1">0.663</td><td colspan="1" rowspan="1">0.620</td><td colspan="1" rowspan="1">0.616</td><td colspan="1" rowspan="1">0.834</td><td colspan="1" rowspan="1">0.375</td><td colspan="1" rowspan="1">0.349</td><td colspan="1" rowspan="1">0.337</td></tr><tr><td colspan="1" rowspan="1">DeepPCT<sup>AF3</sup></td><td colspan="1" rowspan="1">0.956</td><td colspan="1" rowspan="1">0.662</td><td colspan="1" rowspan="1">0.627</td><td colspan="1" rowspan="1">0.627</td><td colspan="1" rowspan="1">0.838</td><td colspan="1" rowspan="1">0.381</td><td colspan="1" rowspan="1">0.357</td><td colspan="1" rowspan="1">0.348</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><p>AF3 denotes that AlphaFold3-based predicted structures were used.</p></fn><fn id="tblfn2"><p>DeepPCTseq<sup>nrp</sup> denotes that the residue pair embedding was deleted in DeepPCTseq.</p></fn></table-wrap-foot></table-wrap><p>Meanwhile, we constructed baseline models for each basic classifier (<xref ref-type="supplementary-material" rid="sup1">Supplementary Text S4</xref>). Relative to our two deep learning classifiers (i.e. DeepPCTseq and DeepPCTgraph), we established baselines using the convolutional neural network (CNN) and long-short term memory network (LSTM). For the graph-based classifier, we constructed three additional baselines using graph convolutional network (GCN), graph attention network (GAT), and GraphSAGE. For the machine learning classifier (i.e. DeepPCTsite), we additionally evaluated five commonly used machine learning algorithms. As shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S4</xref>, all the baseline models were inferior to the finally selected models. Notably, the CNN and LSTM baselines for DeepPCTgraph yielded relatively worse results, whereas the graph-based baselines generated a slightly declined performance. This is probably because the CNN and LSTM models cannot handle the graph information, suggesting the importance of constructing graph representations for PTM pairs in our approach.</p><p>Additionally, we explored the interpretability of representative features adopted in each classifier. Recently, Rao <italic toggle="yes">et al.</italic> demonstrated that the attention scores from PLMs can accurately predict residue contacts in proteins through a simple logistic regression method (<xref ref-type="bibr" rid="btae675-B25">Rao <italic toggle="yes">et al.</italic> 2020</xref>). Inspired by this work, we utilized the residue pair embedding to characterize the association between PTM sites for sequence-based prediction. To analyze this feature, we applied their method to transform the embeddings into single contact scores and compared the scores of positive and negative samples. It is clear that the contact scores of crosstalk pairs were generally higher in relevant proteins (<xref ref-type="fig" rid="btae675-F2">Fig. 2A−D</xref>), and there was a significant difference between all the positive and negative samples (<xref ref-type="fig" rid="btae675-F2">Fig. 2E</xref>, <italic toggle="yes">P</italic> = 1.97e−28), implying the potential of the residue pair embedding to identify crosstalk pairs. As shown in <xref ref-type="table" rid="btae675-T1">Table 1</xref>, excluding this embedding resulted in a remarkable decrease in the performance of sequence-based classifiers (DeepPCTseq<sup>nrp</sup> versus DeepPCTseq). For the structure-based prediction, a group of hand-crafted descriptors were newly introduced to complement the structural embedding. We found that crosstalk pairs tended to be located on the protein surface (<xref ref-type="fig" rid="btae675-F3">Fig. 3A, B, and F</xref>), be within the shallower region of protein pockets (<xref ref-type="fig" rid="btae675-F3">Fig. 3C</xref>), and be spatially proximate in protein structures (<xref ref-type="fig" rid="btae675-F3">Fig. 3H</xref>). This may be because residues that are deeply buried or quite distant from each other could hinder their interactions. Moreover, the OPD chirality indices of crosstalk pairs were significantly lower than those of negative samples (<xref ref-type="fig" rid="btae675-F3">Fig. 3G</xref>). This measure could be correlated with the secondary structure status of a residue (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S2</xref>). Namely, the crosstalk pairs were enriched in the coil region, where residues generally have smaller chirality index values. The above results suggest these features could capture certain patterns to distinguish crosstalk pairs from negative pairs.</p><fig id="btae675-F2" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>Comparison of predicted contact scores of crosstalk and negative pairs based on residue pair embeddings. (A-D) Visualization of predicted contact score and ground truth for representative examples. UniProt IDs: (A) <ext-link ext-link-type="pmc:entrez-protein" xlink:href="P24864" xmlns:xlink="http://www.w3.org/1999/xlink">P24864</ext-link>, (B) <ext-link ext-link-type="pmc:entrez-protein" xlink:href="Q15653" xmlns:xlink="http://www.w3.org/1999/xlink">Q15653</ext-link>, (C) <ext-link ext-link-type="pmc:entrez-protein" xlink:href="Q9UKT4" xmlns:xlink="http://www.w3.org/1999/xlink">Q9UKT4</ext-link>, and (D) <ext-link ext-link-type="pmc:entrez-protein" xlink:href="Q99741" xmlns:xlink="http://www.w3.org/1999/xlink">Q99741</ext-link>. (E) Distribution of predicted contact scores for crosstalk and negative pairs.</p></caption><graphic orientation="portrait" position="float" xlink:href="btae675f2.jpg" xmlns:xlink="http://www.w3.org/1999/xlink"/></fig><fig id="btae675-F3" orientation="portrait" position="float"><label>Figure 3.</label><caption><p>Comparison of structural features between crosstalk (red) and negative pairs (blue). (A) Accessible shell volume. (B) Minimum inaccessible radius. (C) Pocketness. (D) Ollivier-Ricci curvature. (E) Multifractal dimension. (F) Circular variance. (G) OPD chirality index (calculated using <italic toggle="yes">N</italic> = 5, 7, 10, and 15). (H) Shortest path distance. Max, Min, and Ave represent the maximum, minimum, and average values of two residues in each PTM pair regarding a given attribute. The statistical significance is evaluated using the Wilcoxon rank sum test. ***<italic toggle="yes">P</italic> &lt; 0.001, **<italic toggle="yes">P</italic> &lt; 0.01, and *<italic toggle="yes">P</italic> &lt; 0.05.</p></caption><graphic orientation="portrait" position="float" xlink:href="btae675f3.jpg" xmlns:xlink="http://www.w3.org/1999/xlink"/></fig></sec><sec><title>3.2 Integration of basic classifiers improves prediction performance</title><p>To explore the complementarity among three basic classifiers, we evaluated the performance of integrated modules (i.e. DeepPCTstr and DeepPCT) on the training set. As shown in <xref ref-type="fig" rid="btae675-F4">Fig. 4A</xref> and <xref ref-type="table" rid="btae675-T1">Table 1</xref>, although DeepPCTsite achieved the lowest measures among basic classifiers, it provided a complement to the other structure-based classifier. Through combining DeepPCTgraph with DeepPCTsite, even for the stringent protein-based evaluation, the AUC and AUPR values were improved from 0.739 and 0.182 to 0.786 and 0.217, respectively. This suggests that the hand-crafted descriptors effectively complement the embedding features in capturing the structural patterns of crosstalk sites. When we further integrated DeepPCTseq with DeepPCTstr, the AUC and AUPR values increased from 0.827 and 0.368 to 0.834 and 0.375, respectively. <xref ref-type="fig" rid="btae675-F4">Figure 4B</xref> illustrates that both sequence- and structure-based modules achieved better MCC values for partial proteins. The Pearson correlation coefficients between their results were 0.406 and 0.570 for protein- and sample-based evaluations, respectively, implying that these modules could complement each other. For instance, the number of true positives in the histone H3 gradually increased with the integration of various classifiers in the sample-based evaluation (<xref ref-type="fig" rid="btae675-F4">Fig. 4C</xref>). Finally, DeepPCT successfully identified 20 pairs among the 22 positive samples, while yielding 6 false positives out of 34 negative samples. For the protein-based evaluation, nevertheless, DeepPCTseq performed poorly on this protein, generating only three true positives (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S3</xref>). After including DeepPCTstr, the number of true positives increased from three to nine (DeepPCTseq versus DeepPCT), while that of false positives decreased from five to one (DeepPCTstr versus DeepPCT). This further indicates that the sequence- and structure-based modules could provide different information. Besides, we tried different strategies to integrate the sequence- and structure-based information into a deep learning framework, but these attempts failed to surpass the weighted combination approach (<xref ref-type="supplementary-material" rid="sup1">Supplementary Text S5</xref> and <xref ref-type="supplementary-material" rid="sup1">Table S5</xref>).</p><fig id="btae675-F4" orientation="portrait" position="float"><label>Figure 4.</label><caption><p>Comparative analysis of three basic classifiers and their integrations. (A) ROC (left) and precision-recall (right) curves for DeepPCT and its modules on training set. (B) Scatter plots of MCC on training set using protein- and sample-based evaluations. Each point denotes the MCC value of all samples in a given protein. (C) Visualization of prediction results of PTM pairs in histone H3 using sample-based evaluation. True positives, false positives, and false negatives are highlighted in red, blue, and gray, respectively.</p></caption><graphic orientation="portrait" position="float" xlink:href="btae675f4.jpg" xmlns:xlink="http://www.w3.org/1999/xlink"/></fig></sec><sec><title>3.3 DeepPCT exhibits less distance dependency than other methods</title><p>PTM sites involved in crosstalk events are generally proximate to each other at both the sequence and structural levels. Our previous study has indicated that the residue pair-based attributes (e.g. distance, co-evolution, and co-localization), which strongly depend on the distance between PTMs, might have difficulty in identifying remote crosstalk sites and adjacent unrelated sites (<xref ref-type="bibr" rid="btae675-B20">Liu and Liu 2020</xref>). To investigate this dependency, here, we built a random forest predictor for each feature used by our and other studies and assessed the performance on three datasets, including a randomly sampled dataset (i.e. Set-random) and two distance-constrained datasets (i.e. Seq-control and Str-control). Only the shortest path distance exhibited a remarkable performance decrease on distance-constrained samples in the present method (<xref ref-type="fig" rid="btae675-F5">Fig. 5A</xref>). Additionally, the descriptors used by DeepPCT yielded a much less average loss than those used by existing methods (<xref ref-type="fig" rid="btae675-F5">Fig. 5B</xref>). Among the features adopted in this work, the residue embedding, residue pair embedding, and structural embedding obtained the highest performance on Set-random, with an AUC of approximately 0.880, while the AUCs of other descriptors ranged from 0.613 to 0.752. Particularly, the residue and structural embeddings achieved robust results on distance-constrained datasets, with a decrease of approximately 0.008 and 0.001 in AUC, respectively, compared with a value of approximately 0.05 for the residue pair embedding. The remarkable decline could be due to the following reasons. First, the sequence distance could be implicitly involved in this feature during the positional encoding of the ESM-2 model (<xref ref-type="bibr" rid="btae675-B19">Lin <italic toggle="yes">et al.</italic> 2023</xref>). Second, the attention scores of PLMs could be considered as the high-level representation of residue co-evolution (<xref ref-type="bibr" rid="btae675-B30">Vig <italic toggle="yes">et al.</italic> 2021</xref>). In contrast, the residue pair-based features from other methods exhibited much more drastic decreases. For example, the AUCs of co-evolutionary features of PCTpred and PTM-X were reduced from 0.711 and 0.657 to 0.526 and 0.517, respectively (<xref ref-type="supplementary-material" rid="sup1">Supplementary Table S6</xref>). Collectively, our descriptors generally have less distance dependencies, and DeepPCT could be more robust in different predictive scenarios.</p><fig id="btae675-F5" orientation="portrait" position="float"><label>Figure 5.</label><caption><p>Analysis of distance dependency of different features and methods. (A) Performance of individual features of DeepPCT on Set-random and Seq/Str-control. (B) Average performance of sequence and structural features used by DeepPCT and other methods on Set-random and Seq/Str-control.</p></caption><graphic orientation="portrait" position="float" xlink:href="btae675f5.jpg" xmlns:xlink="http://www.w3.org/1999/xlink"/></fig></sec><sec><title>3.4 DeepPCT outperforms previous methods</title><p>To further demonstrate the advantages of our approach, we compared DeepPCT with existing machine learning-based methods. Note that we retrained the competing methods using the AlphaFold2-based structures of our training set and applied them to the newly collected testing set. As shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Tables S7–S10</xref>, DeepPCT yielded optimal performance for both sample- and protein-based evaluations on the training set, followed by PCTpred and PTM-X. For the testing set, our algorithm surpassed PCTpred and PTM-X by 0.082 and 0.095 in AUC and 0.075 and 0.088 in AUPR, respectively (<xref ref-type="table" rid="btae675-T2">Table 2</xref>). As discussed previously, if a model is highly dependent on the distance between PTMs, its generalizability could become weak. All the five features of PTM-X exhibited a strong distance dependency, but the distance difference between the positive and negative samples of the testing set (sequence distance: <italic toggle="yes">P</italic> = 1.66e−2) was much smaller than that of the training set (sequence distance: <italic toggle="yes">P</italic> = 4.43e−59). This led to the worst performance generated by PTM-X. In contrast, DeepPCT and PCTpred, which were less dependent on the distance measure, tended to perform more stably on new data. Besides, DeepPCT may also benefit from the protein-based optimization strategy. Previous methods relied on the sample-based optimization and lacked a rigorous independent testing, thus having the potential issue of overfitting. We still observed the complementarity between the three classifiers of DeepPCT on testing samples. The combination of DeepPCTsite and DeepPCTgraph induced an increment of approximately 0.03 in AUC, while the integration of DeepPCTstr and DeepPCTseq resulted in an increase of approximately 0.01. However, this tendency was not observed for other methods. For instance, PTM-X performed even worse than PTM-Xseq on testing data (AUC: 0.667 versus 0.688). This is because the difference in structural distance between positive and negative pairs was less significant (<italic toggle="yes">P</italic> = 0.466). Therefore, PTM-Xstr, which only adopted the structural distance as input, achieved very poor results (AUC: 0.543), and incorporation of this feature exerted a negative effect. This further suggests the importance of reducing distance dependency.</p><table-wrap id="btae675-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Comparison between DeepPCT and existing methods on testing set.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/><col align="char" char="." span="1" valign="top"/></colgroup><thead><tr><th colspan="1" rowspan="1">Method</th><th align="center" colspan="1" rowspan="1">AUC</th><th align="center" colspan="1" rowspan="1">AUPR</th><th align="center" colspan="1" rowspan="1">MCC</th><th align="center" colspan="1" rowspan="1">F1</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">DeepPCTsite</td><td colspan="1" rowspan="1">0.684</td><td colspan="1" rowspan="1">0.130</td><td colspan="1" rowspan="1">0.129</td><td colspan="1" rowspan="1">0.171</td></tr><tr><td colspan="1" rowspan="1">DeepPCTsite<sup>AF3</sup></td><td colspan="1" rowspan="1">0.754</td><td colspan="1" rowspan="1">0.173</td><td colspan="1" rowspan="1">0.175</td><td colspan="1" rowspan="1">0.193</td></tr><tr><td colspan="1" rowspan="1">DeepPCTgraph</td><td colspan="1" rowspan="1">0.722</td><td colspan="1" rowspan="1">0.106</td><td colspan="1" rowspan="1">0.127</td><td colspan="1" rowspan="1">0.169</td></tr><tr><td colspan="1" rowspan="1">DeepPCTgraph<sup>AF3</sup></td><td colspan="1" rowspan="1">0.735</td><td colspan="1" rowspan="1">0.186</td><td colspan="1" rowspan="1">0.175</td><td colspan="1" rowspan="1">0.194</td></tr><tr><td colspan="1" rowspan="1">DeepPCTstr</td><td colspan="1" rowspan="1">0.754</td><td colspan="1" rowspan="1">0.127</td><td colspan="1" rowspan="1">0.156</td><td colspan="1" rowspan="1">0.197</td></tr><tr><td colspan="1" rowspan="1">DeepPCTstr<sup>AF3</sup></td><td colspan="1" rowspan="1">0.765</td><td colspan="1" rowspan="1">0.245</td><td colspan="1" rowspan="1">0.220</td><td colspan="1" rowspan="1">0.242</td></tr><tr><td colspan="1" rowspan="1">DeepPCTseq</td><td colspan="1" rowspan="1">0.733</td><td colspan="1" rowspan="1">0.238</td><td colspan="1" rowspan="1">0.209</td><td colspan="1" rowspan="1">0.167</td></tr><tr><td colspan="1" rowspan="1">DeepPCT</td><td colspan="1" rowspan="1">0.762</td><td colspan="1" rowspan="1">0.240</td><td colspan="1" rowspan="1">0.210</td><td colspan="1" rowspan="1">0.235</td></tr><tr><td colspan="1" rowspan="1">DeepPCT<sup>AF3</sup></td><td colspan="1" rowspan="1">0.777</td><td colspan="1" rowspan="1">0.278</td><td colspan="1" rowspan="1">0.245</td><td colspan="1" rowspan="1">0.258</td></tr><tr><td colspan="1" rowspan="1">PCTseq</td><td colspan="1" rowspan="1">0.664</td><td colspan="1" rowspan="1">0.182</td><td colspan="1" rowspan="1">0.123</td><td colspan="1" rowspan="1">0.168</td></tr><tr><td colspan="1" rowspan="1">PCTstr</td><td colspan="1" rowspan="1">0.676</td><td colspan="1" rowspan="1">0.137</td><td colspan="1" rowspan="1">0.132</td><td colspan="1" rowspan="1">0.172</td></tr><tr><td colspan="1" rowspan="1">PCTstr<sup>AF3</sup></td><td colspan="1" rowspan="1">0.655</td><td colspan="1" rowspan="1">0.125</td><td colspan="1" rowspan="1">0.152</td><td colspan="1" rowspan="1">0.191</td></tr><tr><td colspan="1" rowspan="1">PCTpred</td><td colspan="1" rowspan="1">0.680</td><td colspan="1" rowspan="1">0.165</td><td colspan="1" rowspan="1">0.154</td><td colspan="1" rowspan="1">0.195</td></tr><tr><td colspan="1" rowspan="1">PCTpred <sup>AF3</sup></td><td colspan="1" rowspan="1">0.678</td><td colspan="1" rowspan="1">0.151</td><td colspan="1" rowspan="1">0.161</td><td colspan="1" rowspan="1">0.202</td></tr><tr><td colspan="1" rowspan="1">PTM-Xseq</td><td colspan="1" rowspan="1">0.688</td><td colspan="1" rowspan="1">0.166</td><td colspan="1" rowspan="1">0.233</td><td colspan="1" rowspan="1">0.252</td></tr><tr><td colspan="1" rowspan="1">PTM-Xstr</td><td colspan="1" rowspan="1">0.543</td><td colspan="1" rowspan="1">0.075</td><td colspan="1" rowspan="1">0.106</td><td colspan="1" rowspan="1">0.151</td></tr><tr><td colspan="1" rowspan="1">PTM-Xstr <sup>AF3</sup></td><td colspan="1" rowspan="1">0.528</td><td colspan="1" rowspan="1">0.112</td><td colspan="1" rowspan="1">0.105</td><td colspan="1" rowspan="1">0.151</td></tr><tr><td colspan="1" rowspan="1">PTM-X</td><td colspan="1" rowspan="1">0.667</td><td colspan="1" rowspan="1">0.152</td><td colspan="1" rowspan="1">0.122</td><td colspan="1" rowspan="1">0.166</td></tr><tr><td colspan="1" rowspan="1">PTM-X <sup>AF3</sup></td><td colspan="1" rowspan="1">0.671</td><td colspan="1" rowspan="1">0.156</td><td colspan="1" rowspan="1">0.126</td><td colspan="1" rowspan="1">0.167</td></tr></tbody></table><table-wrap-foot><fn id="tblfn3"><p>AF3 denotes that AlphaFold3-based predicted structures were used.</p></fn><fn id="tblfn4"><p>PCTseq and PCTstr are the sequence and structure-based predictors of PCTpred, respectively.</p></fn><fn id="tblfn5"><p>PTM-Xseq and PTM-Xstr denote only using sequence and structural features of PTM-X, respectively.</p></fn></table-wrap-foot></table-wrap><p>As shown in <xref ref-type="table" rid="btae675-T2">Table 2</xref>, the individual and integrated modules of DeepPCT were superior to the corresponding parts of PCTpred. Thus, we summarize the advantage of current method over our previous algorithm (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S4</xref>). Regarding the sequence-based module, the residue and residue pair embeddings generated by PLMs were utilized to substitute the hand-crafted residue- and residue pair-based features. Meanwhile, the prediction engine was updated from a random forest model to a deep learning model, which used the attention mechanism to effectively extract residue representations from sequence windows. Regarding the structure-based module, we designed not only a machine learning predictor but also a deep learning predictor. Following the design concept of DeepPCTseq, we adopted the structural embedding of spatial windows of PTM pairs as the input of a graph neural network to implement DeepPCTgraph. In the machine learning component (DeepPCTsite), we retained the framework of PCTstr but used a group of novel hand-crafted features and reduced residue pair-based features to mitigate the distance dependency. Regarding the ensemble module, our current algorithm was the mixed combination of two deep learning predictors and one machine learning predictor, whereas our previous method was the pure integration of two machine learning predictors. Additionally, DeepPCT had higher computational efficiency than PCTpred. The inference time of DeepPCT for the testing set was approximately 10 minutes, compared to 10 hours for PCTpred (tested on Intel Xeon Silver 4116 T CPU).</p><p>When we are preparing this manuscript, AlphaFold3 was released with extended capabilities and improved performances (<xref ref-type="bibr" rid="btae675-B1">Abramson <italic toggle="yes">et al.</italic> 2024</xref>). We therefore re-evaluated our method using AlphaFold3-based structures. As shown in <xref ref-type="table" rid="btae675-T1">Tables 1</xref> and <xref ref-type="table" rid="btae675-T2">2</xref>, the performance of the structure-based predictors and the finally integrated model was slightly improved on the training and testing sets. Besides, certain modules achieved a marked increase in specific measures on testing samples (e.g. AUC: 0.684 versus 0.754 for DeepPCTsite). By comparing the structural similarities between native and predicted structures of 59 proteins in our datasets using the TM-align program (<xref ref-type="bibr" rid="btae675-B36">Zhang and Skolnick 2005</xref>), we found that AlphaFold3-based structures were closer to native structures than AlphaFold2-based counterparts (average TM-score: 0.763 versus 0.756). Thus, the increase in performance could be attributed to the improved accuracy of predicted structures. Collectively, as the quality of predicted structures increases, the utility of DeepPCT could be further enhanced.</p></sec></sec><sec><title>4 Discussion</title><p>Although machine learning-based methods have been developed to predict PTM crosstalk within proteins, the prediction performance remains to be further improved. Application of deep learning to this field would be an emerging and promising way to solve this problem. Besides, the availability of high-quality predicted structures could facilitate the development of prediction models. Accordingly, we proposed DeepPCT, which is a deep learning framework to identify the interplay between PTMs based on AlphaFold2-based predicted structures. Our algorithm comprised two deep learning classifiers for sequence- and structure-based predictions, respectively, both of which took the embedding vector derived from pre-trained models as inputs. Meanwhile, a group of hand-crafted descriptors were newly applied to establish another structure-based classifier. Comprehensive evaluations indicate that the three classifiers offered complementary information and the ensemble model can yield the best measures for both sample- and protein-based assessments. The usage of predicted structures in this work implies that the application range of our method may be broad. Compared with existing methods, DeepPCT had better performance in different scenarios, especially showing strong generalizability on newly collected data.</p><p>Despite these progresses achieved here, several problems are worthy of studying in the future work. First, we only used the original structures to develop our model and neglected the information of structures with modified sites. Recent advances (e.g. RoseTTAFold All-Atom and AlphaFold3) enable the prediction of protein structures with PTMs (<xref ref-type="bibr" rid="btae675-B1">Abramson <italic toggle="yes">et al.</italic> 2024</xref>, <xref ref-type="bibr" rid="btae675-B17">Krishna <italic toggle="yes">et al.</italic> 2024</xref>). Therefore, the modified structures could be adopted to extract indicators of crosstalk events. Second, although the deep learning and embedding features had clearly improved the performance, the interpretability became weak compared to that of machine learning and traditional features. The learned patterns need to be further elucidated. Third, the present algorithm could only evaluate whether a PTM pair is involved in crosstalk events. As more suitable data become available, we will develop models to predict the interaction type of associated PTM sites (i.e. facilitate, inhibit, and co-operate) (<xref ref-type="bibr" rid="btae675-B10">Huang <italic toggle="yes">et al.</italic> 2015</xref>). Fourth, due to the limitation of existing experimental techniques, our knowledge of proteome-wide crosstalk events is very limited. Leveraging the inference efficiency of our algorithm, we could apply DeepPCT to all human protein structures in the AlphaFold database, which would provide an alternative avenue for investigating the proteome landscape and structural patterns of PTM crosstalk events. In summary, DeepPCT is an effective tool to explore the relationship between PTMs, which could advance our knowledge of crosstalk events in proteins.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="sup1" orientation="portrait" position="float"><label>btae675_Supplementary_Data</label><media orientation="portrait" position="float" xlink:href="btae675_supplementary_data.pdf" xmlns:xlink="http://www.w3.org/1999/xlink"/></supplementary-material></sec></body><back><sec><title>Supplementary data</title><p>
<xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p><p>Conflict of interest: None declared.</p></sec><sec><title>Funding</title><p>This work was supported by the National Natural Science Foundation of China [32071249 to R.L.].</p></sec><ref-list id="ref1"><title>References</title><ref id="btae675-B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Abramson</surname><given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Adler</surname><given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Dunger</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Accurate structure prediction of biomolecular interactions with AlphaFold 3</article-title>. <source>Nature</source><year>2024</year>;<volume>630</volume>:<fpage>493</fpage>–<lpage>500</lpage>.<pub-id pub-id-type="pmid">38718835</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1038/s41586-024-07487-w</pub-id><pub-id pub-id-type="pmcid">PMC11168924</pub-id></mixed-citation></ref><ref id="btae675-B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Adoni</surname><given-names>KR</given-names></string-name>, <string-name name-style="western"><surname>Cunningham</surname><given-names>DL</given-names></string-name>, <string-name name-style="western"><surname>Heath</surname><given-names>JK</given-names></string-name></person-group><etal>et al</etal><article-title>FAIMS enhances the detection of PTM crosstalk sites</article-title>. <source>J Proteome Res</source><year>2022</year>;<volume>21</volume>:<fpage>930</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">35235327</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1021/acs.jproteome.1c00721</pub-id><pub-id pub-id-type="pmcid">PMC8981314</pub-id></mixed-citation></ref><ref id="btae675-B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Beltrao</surname><given-names>P</given-names></string-name>, <string-name name-style="western"><surname>Bork</surname><given-names>P</given-names></string-name>, <string-name name-style="western"><surname>Krogan</surname><given-names>NJ</given-names></string-name></person-group><etal>et al</etal><article-title>Evolution and functional cross-talk of protein post-translational modifications</article-title>. <source>Mol Syst Biol</source><year>2013</year>;<volume>9</volume>:<fpage>714</fpage>.<pub-id pub-id-type="pmid">24366814</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1002/msb.201304521</pub-id><pub-id pub-id-type="pmcid">PMC4019982</pub-id></mixed-citation></ref><ref id="btae675-B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Breiman</surname><given-names>L.</given-names></string-name></person-group><article-title>Random forests</article-title>. <source>Mach Learn</source><year>2001</year>;<volume>45</volume>:<fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation></ref><ref id="btae675-B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Chen</surname><given-names>L</given-names></string-name>, <string-name name-style="western"><surname>Liu</surname><given-names>S</given-names></string-name>, <string-name name-style="western"><surname>Tao</surname><given-names>Y.</given-names></string-name></person-group><article-title>Regulating tumor suppressor genes: post-translational modifications</article-title>. <source>Signal Transduct Target Ther</source><year>2020</year>;<volume>5</volume>:<fpage>90</fpage>.<pub-id pub-id-type="pmid">32532965</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1038/s41392-020-0196-9</pub-id><pub-id pub-id-type="pmcid">PMC7293209</pub-id></mixed-citation></ref><ref id="btae675-B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Deng</surname><given-names>L</given-names></string-name>, <string-name name-style="western"><surname>Zhu</surname><given-names>F</given-names></string-name>, <string-name name-style="western"><surname>He</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>Prediction of post-translational modification cross-talk and mutation within proteins via imbalanced learning</article-title>. <source>Expert Syst Appl</source><year>2023</year>;<volume>211</volume>:<fpage>118593</fpage>.</mixed-citation></ref><ref id="btae675-B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Fischle</surname><given-names>W</given-names></string-name>, <string-name name-style="western"><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name name-style="western"><surname>David Allis</surname><given-names>C.</given-names></string-name></person-group><article-title>Binary switches and modification cassettes in histone biology and beyond</article-title>. <source>Nature</source><year>2003</year>;<volume>425</volume>:<fpage>475</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">14523437</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1038/nature02017</pub-id></mixed-citation></ref><ref id="btae675-B8"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>He</surname><given-names>K</given-names></string-name>, <string-name name-style="western"><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name name-style="western"><surname>Ren</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>Deep Residual Learning for Image Recognition</article-title>. In: <source>IEEE Conference on Computer Vision and Pattern Recognition</source>, Las Vegas, NV, USA, <year>2016</year>.</mixed-citation></ref><ref id="btae675-B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Hornbeck</surname><given-names>PV</given-names></string-name>, <string-name name-style="western"><surname>Zhang</surname><given-names>B</given-names></string-name>, <string-name name-style="western"><surname>Murray</surname><given-names>B</given-names></string-name></person-group><etal>et al</etal><article-title>PhosphoSitePlus, 2014: mutations, PTMs and recalibrations</article-title>. <source>Nucleic Acids Res</source><year>2015</year>;<volume>43</volume>:<fpage>D512</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">25514926</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1093/nar/gku1267</pub-id><pub-id pub-id-type="pmcid">PMC4383998</pub-id></mixed-citation></ref><ref id="btae675-B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name name-style="western"><surname>Xu</surname><given-names>B</given-names></string-name>, <string-name name-style="western"><surname>Zhou</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal><article-title>Systematic characterization and prediction of post-translational modification cross-talk</article-title>. <source>Mol Cell Proteomics</source><year>2015</year>;<volume>14</volume>:<fpage>761</fpage>–<lpage>70</lpage>.<pub-id pub-id-type="pmid">25605461</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1074/mcp.M114.037994</pub-id><pub-id pub-id-type="pmcid">PMC4349993</pub-id></mixed-citation></ref><ref id="btae675-B11"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Ishida</surname><given-names>T</given-names></string-name>, <string-name name-style="western"><surname>Yamane</surname><given-names>I</given-names></string-name>, <string-name name-style="western"><surname>Sakai</surname><given-names>T</given-names></string-name></person-group><etal>et al</etal><article-title>Do we need zero training loss after achieving zero training error?</article-title> In: <source>International Conference on Machine Learning</source>, Virtual Event, <year>2020</year>.</mixed-citation></ref><ref id="btae675-B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Jiang</surname><given-names>Z</given-names></string-name>, <string-name name-style="western"><surname>Shen</surname><given-names>YY</given-names></string-name>, <string-name name-style="western"><surname>Liu</surname><given-names>R.</given-names></string-name></person-group><article-title>Structure-based prediction of nucleic acid binding residues by merging deep learning- and template-based approaches</article-title>. <source>PLoS Comput Biol</source><year>2023</year>;<volume>19</volume>:<fpage>e1011428</fpage>.<pub-id pub-id-type="pmid">37672551</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1371/journal.pcbi.1011428</pub-id><pub-id pub-id-type="pmcid">PMC10482303</pub-id></mixed-citation></ref><ref id="btae675-B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Jumper</surname><given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Evans</surname><given-names>R</given-names></string-name>, <string-name name-style="western"><surname>Pritzel</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal><article-title>Highly accurate protein structure prediction with AlphaFold</article-title>. <source>Nature</source><year>2021</year>;<volume>596</volume>:<fpage>583</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">34265844</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id><pub-id pub-id-type="pmcid">PMC8371605</pub-id></mixed-citation></ref><ref id="btae675-B14"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Kingma</surname><given-names>DP</given-names></string-name>, <string-name name-style="western"><surname>Ba</surname><given-names>J.</given-names></string-name></person-group><article-title>Adam: A Method for Stochastic Optimization</article-title>. In: <source>International Conference on Learning Representations</source>, Banff, Canada, <year>2014</year>.</mixed-citation></ref><ref id="btae675-B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Korkuc</surname><given-names>P</given-names></string-name>, <string-name name-style="western"><surname>Walther</surname><given-names>D.</given-names></string-name></person-group><article-title>Spatial proximity statistics suggest a regulatory role of protein phosphorylation on compound binding</article-title>. <source>Proteins</source><year>2016</year>;<volume>84</volume>:<fpage>565</fpage>–<lpage>79</lpage>.<pub-id pub-id-type="pmid">26817627</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1002/prot.25001</pub-id></mixed-citation></ref><ref id="btae675-B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Korkuc</surname><given-names>P</given-names></string-name>, <string-name name-style="western"><surname>Walther</surname><given-names>D.</given-names></string-name></person-group><article-title>Towards understanding the crosstalk between protein post-translational modifications: homo- and heterotypic PTM pair distances on protein surfaces are not random</article-title>. <source>Proteins</source><year>2017</year>;<volume>85</volume>:<fpage>78</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">27802577</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1002/prot.25200</pub-id></mixed-citation></ref><ref id="btae675-B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Krishna</surname><given-names>R</given-names></string-name>, <string-name name-style="western"><surname>Wang</surname><given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Ahern</surname><given-names>W</given-names></string-name></person-group><etal>et al</etal><article-title>Generalized biomolecular modeling and design with RoseTTAFold All-Atom</article-title>. <source>Science</source><year>2024</year>;<volume>384</volume>:<fpage>eadl2528</fpage>.<pub-id pub-id-type="pmid">38452047</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1126/science.adl2528</pub-id></mixed-citation></ref><ref id="btae675-B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Kulmanov</surname><given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Guzmán-Vega</surname><given-names>FJ</given-names></string-name>, <string-name name-style="western"><surname>Duek Roggli</surname><given-names>P</given-names></string-name></person-group><etal>et al</etal><article-title>Protein function prediction as approximate semantic entailment</article-title>. <source>Nat Mach Intell</source><year>2024</year>;<volume>6</volume>:<fpage>220</fpage>–<lpage>8</lpage>.</mixed-citation></ref><ref id="btae675-B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Lin</surname><given-names>Z</given-names></string-name>, <string-name name-style="western"><surname>Akin</surname><given-names>H</given-names></string-name>, <string-name name-style="western"><surname>Rao</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>Evolutionary-scale prediction of atomic-level protein structure with a language model</article-title>. <source>Science</source><year>2023</year>;<volume>379</volume>:<fpage>1123</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">36927031</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1126/science.ade2574</pub-id></mixed-citation></ref><ref id="btae675-B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Liu</surname><given-names>H-F</given-names></string-name>, <string-name name-style="western"><surname>Liu</surname><given-names>R.</given-names></string-name></person-group><article-title>Structure-based prediction of post-translational modification cross-talk within proteins using complementary residue- and residue pair-based features</article-title>. <source>Brief Bioinform</source><year>2020</year>;<volume>21</volume>:<fpage>609</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">30649184</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1093/bib/bby123</pub-id></mixed-citation></ref><ref id="btae675-B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Liu</surname><given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Qian</surname><given-names>C</given-names></string-name>, <string-name name-style="western"><surname>Cao</surname><given-names>X.</given-names></string-name></person-group><article-title>Post-Translational modification control of innate immunity</article-title>. <source>Immunity</source><year>2016</year>;<volume>45</volume>:<fpage>15</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">27438764</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1016/j.immuni.2016.06.020</pub-id></mixed-citation></ref><ref id="btae675-B22"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Paszke</surname><given-names>A</given-names></string-name>, <string-name name-style="western"><surname>Gross</surname><given-names>S</given-names></string-name>, <string-name name-style="western"><surname>Massa</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title>. In: <source>Advances in Neural Information Processing Systems</source>, Vancouver, Canada, <year>2019</year>.</mixed-citation></ref><ref id="btae675-B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Pedregosa</surname><given-names>F</given-names></string-name>, <string-name name-style="western"><surname>Varoquaux</surname><given-names>G</given-names></string-name>, <string-name name-style="western"><surname>Gramfort</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal><article-title>Scikit-learn: machine learning in Python</article-title>. <source>Journal of Machine Learning Research</source><year>2011</year>;<volume>12</volume>:<fpage>2825</fpage>–<lpage>30</lpage>.</mixed-citation></ref><ref id="btae675-B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Pejaver</surname><given-names>V</given-names></string-name>, <string-name name-style="western"><surname>Hsu</surname><given-names>W-L</given-names></string-name>, <string-name name-style="western"><surname>Xin</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><article-title>The structural and functional signatures of proteins that undergo multiple events of post-translational modification</article-title>. <source>Protein Sci</source><year>2014</year>;<volume>23</volume>:<fpage>1077</fpage>–<lpage>93</lpage>.<pub-id pub-id-type="pmid">24888500</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1002/pro.2494</pub-id><pub-id pub-id-type="pmcid">PMC4116656</pub-id></mixed-citation></ref><ref id="btae675-B25"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Rao</surname><given-names>R</given-names></string-name>, <string-name name-style="western"><surname>Meier</surname><given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Sercu</surname><given-names>T</given-names></string-name></person-group><etal>et al</etal> Transformer protein language models are unsupervised structure learners. bioRxiv, <year>2020</year>, preprint: not peer reviewed. https://doi.org/10.1101/2020.12.15.422761</mixed-citation></ref><ref id="btae675-B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Schweiger</surname><given-names>R</given-names></string-name>, <string-name name-style="western"><surname>Linial</surname><given-names>M.</given-names></string-name></person-group><article-title>Cooperativity within proximal phosphorylation sites is revealed from large-scale proteomics data</article-title>. <source>Biol Direct</source><year>2010</year>;<volume>5</volume>:<fpage>6</fpage>.<pub-id pub-id-type="pmid">20100358</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1186/1745-6150-5-6</pub-id><pub-id pub-id-type="pmcid">PMC2828979</pub-id></mixed-citation></ref><ref id="btae675-B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Varadi</surname><given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Anyango</surname><given-names>S</given-names></string-name>, <string-name name-style="western"><surname>Deshpande</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>AlphaFold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models</article-title>. <source>Nucleic Acids Res</source><year>2022</year>;<volume>50</volume>:<fpage>D439</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">34791371</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1093/nar/gkab1061</pub-id><pub-id pub-id-type="pmcid">PMC8728224</pub-id></mixed-citation></ref><ref id="btae675-B28"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Vaswani</surname><given-names>A</given-names></string-name>, <string-name name-style="western"><surname>Shazeer</surname><given-names>N</given-names></string-name>, <string-name name-style="western"><surname>Parmar</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Attention is all you need</article-title>. In: <source>Advances in Neural Information Processing Systems</source>, Long Beach, California, USA, <year>2017</year>.</mixed-citation></ref><ref id="btae675-B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Venne</surname><given-names>AS</given-names></string-name>, <string-name name-style="western"><surname>Kollipara</surname><given-names>L</given-names></string-name>, <string-name name-style="western"><surname>Zahedi</surname><given-names>RP.</given-names></string-name></person-group><article-title>The next level of complexity: crosstalk of posttranslational modifications</article-title>. <source>Proteomics</source><year>2014</year>;<volume>14</volume>:<fpage>513</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">24339426</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1002/pmic.201300344</pub-id></mixed-citation></ref><ref id="btae675-B30"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Vig</surname><given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Madani</surname><given-names>A</given-names></string-name>, <string-name name-style="western"><surname>Varshney</surname><given-names>LR</given-names></string-name></person-group><etal>et al</etal><article-title>BERTology Meets Biology: Interpreting Attention in Protein Language Models</article-title>. In: <source>International Conference on Learning Representations</source>, Virtual Event, <year>2021</year>.</mixed-citation></ref><ref id="btae675-B31"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Wang</surname><given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Zheng</surname><given-names>D</given-names></string-name>, <string-name name-style="western"><surname>Ye</surname><given-names>Z</given-names></string-name></person-group><etal>et al</etal><article-title>Deep graph library: a graph-centric, highly-performant package for graph neural networks</article-title>. arXiv, arXiv:1909.01315, <year>2019</year>, preprint: not peer reviewed.</mixed-citation></ref><ref id="btae675-B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Witze</surname><given-names>ES</given-names></string-name>, <string-name name-style="western"><surname>Old</surname><given-names>WM</given-names></string-name>, <string-name name-style="western"><surname>Resing</surname><given-names>KA</given-names></string-name></person-group><etal>et al</etal><article-title>Mapping protein post-translational modifications with mass spectrometry</article-title>. <source>Nat Methods</source><year>2007</year>;<volume>4</volume>:<fpage>798</fpage>–<lpage>806</lpage>.<pub-id pub-id-type="pmid">17901869</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1038/nmeth1100</pub-id></mixed-citation></ref><ref id="btae675-B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Woodsmith</surname><given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Kamburov</surname><given-names>A</given-names></string-name>, <string-name name-style="western"><surname>Stelzl</surname><given-names>U.</given-names></string-name></person-group><article-title>Dual coordination of post translational modifications in human protein networks</article-title>. <source>PLoS Comput Biol</source><year>2013</year>;<volume>9</volume>:<fpage>e1002933</fpage>.<pub-id pub-id-type="pmid">23505349</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1371/journal.pcbi.1002933</pub-id><pub-id pub-id-type="pmcid">PMC3591266</pub-id></mixed-citation></ref><ref id="btae675-B34"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Xu</surname><given-names>K</given-names></string-name>, <string-name name-style="western"><surname>Hu</surname><given-names>W</given-names></string-name>, <string-name name-style="western"><surname>Leskovec</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal> How Powerful are Graph Neural Networks? In: <source>International Conference on Learning Representations</source>, New Orleans, USA, <year>2019</year>.</mixed-citation></ref><ref id="btae675-B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Zeng</surname><given-names>Y</given-names></string-name>, <string-name name-style="western"><surname>Wei</surname><given-names>Z</given-names></string-name>, <string-name name-style="western"><surname>Yuan</surname><given-names>Q</given-names></string-name></person-group><etal>et al</etal><article-title>Identifying B-cell epitopes using AlphaFold2 predicted structures and pretrained language model</article-title>. <source>Bioinformatics</source><year>2023</year>;<volume>39</volume>:btad187.<pub-id assigning-authority="pmc" pub-id-type="doi">10.1093/bioinformatics/btad187</pub-id><pub-id pub-id-type="pmcid">PMC10126322</pub-id><pub-id pub-id-type="pmid">37039829</pub-id></mixed-citation></ref><ref id="btae675-B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name name-style="western"><surname>Skolnick</surname><given-names>J.</given-names></string-name></person-group><article-title>TM-align: a protein structure alignment algorithm based on the TM-score</article-title>. <source>Nucleic Acids Res</source><year>2005</year>;<volume>33</volume>:<fpage>2302</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">15849316</pub-id><pub-id assigning-authority="pmc" pub-id-type="doi">10.1093/nar/gki524</pub-id><pub-id pub-id-type="pmcid">PMC1084323</pub-id></mixed-citation></ref><ref id="btae675-B37"><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Zhang</surname><given-names>Z</given-names></string-name>, <string-name name-style="western"><surname>Xu</surname><given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Jamasb</surname><given-names>AR</given-names></string-name></person-group><etal>et al</etal> Protein Representation Learning by Geometric Structure Pretraining. In: <italic toggle="yes">International Conference on Learning Representations</italic>, Kigali, Rwanda, <year>2023</year>.</mixed-citation></ref></ref-list></back></article>